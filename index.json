[{"authors":["liz"],"categories":null,"content":"Welcome! This webpage includes information on my educational background, research areas, current and past projects, my CV, and ways to connect with me.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fc7f296399b51eca9f04506227a81c4e","permalink":"/author/liz-glenn-phd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/liz-glenn-phd/","section":"authors","summary":"Welcome! This webpage includes information on my educational background, research areas, current and past projects, my CV, and ways to connect with me.","tags":null,"title":"Liz Glenn, PhD","type":"authors"},{"authors":null,"categories":null,"content":"","date":1665187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665187200,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"2022-10-08T00:00:00Z","relpermalink":"/about/","section":"","summary":"About Me","tags":null,"title":"ABOUT ME","type":"widget_page"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e\rML Final\rML Final\rLiz G\r12/9/2021\rSet Up\rPackages and Import\rrequire(caret)\r## Loading required package: caret\r## Loading required package: ggplot2\r## Loading required package: lattice\rrequire(recipes)\r## Loading required package: recipes\r## Loading required package: dplyr\r## ## Attaching package: \u0026#39;dplyr\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## filter, lag\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## intersect, setdiff, setequal, union\r## ## Attaching package: \u0026#39;recipes\u0026#39;\r## The following object is masked from \u0026#39;package:stats\u0026#39;:\r## ## step\rrequire(ranger)\r## Loading required package: ranger\rrequire(tidyverse)\r## Loading required package: tidyverse\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v tibble 3.1.5 v purrr 0.3.4\r## v tidyr 1.1.4 v stringr 1.4.0\r## v readr 2.0.2 v forcats 0.5.1\r## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x stringr::fixed() masks recipes::fixed()\r## x dplyr::lag() masks stats::lag()\r## x purrr::lift() masks caret::lift()\rrequire(ModelMetrics)\r## Loading required package: ModelMetrics\r## ## Attaching package: \u0026#39;ModelMetrics\u0026#39;\r## The following objects are masked from \u0026#39;package:caret\u0026#39;:\r## ## confusionMatrix, precision, recall, sensitivity, specificity\r## The following object is masked from \u0026#39;package:base\u0026#39;:\r## ## kappa\rrequire(here)\r## Loading required package: here\r## here() starts at C:/Users/epgle/Desktop/Fall 2021 Classes/EDLD 654 Machine Learning/Class Assignments/Final Project/ML-Final\rrequire(finalfit)\r## Loading required package: finalfit\rrequire(Hmisc)\r## Loading required package: Hmisc\r## Loading required package: survival\r## ## Attaching package: \u0026#39;survival\u0026#39;\r## The following object is masked from \u0026#39;package:caret\u0026#39;:\r## ## cluster\r## Loading required package: Formula\r## ## Attaching package: \u0026#39;Hmisc\u0026#39;\r## The following objects are masked from \u0026#39;package:dplyr\u0026#39;:\r## ## src, summarize\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## format.pval, units\rrequire(rsample)\r## Loading required package: rsample\rrequire(vip)\r## Loading required package: vip\r## ## Attaching package: \u0026#39;vip\u0026#39;\r## The following object is masked from \u0026#39;package:utils\u0026#39;:\r## ## vi\rog_data \u0026lt;- read.csv(here(\u0026quot;data\u0026quot;, \u0026quot;OPP_PRO_Clean.csv\u0026quot;), na.strings = \u0026quot;\u0026quot;)\rog_data \u0026lt;- og_data %\u0026gt;% select(-starts_with(\u0026quot;x\u0026quot;)) %\u0026gt;% #taking out unwanted variables\rselect(-matches(\u0026quot;PSI_total_r|PSI_total_PR\u0026quot;)) %\u0026gt;%\rselect(study_id, PSI_total_clinical, any_of(names(og_data))) %\u0026gt;%\rfilter(!is.na(PSI_total_clinical))\r#taking out unwanted variables, putting in order\rhead(og_data)\r## study_id PSI_total_clinical TC_age_mos PC_age AC_age TC_gender PC_gender\r## 1 OP101 Yes 42 37 40 Male Female\r## 2 OP102 Yes 32 27 NA Female Female\r## 3 OP103 Yes 42 30 29 Male Female\r## 4 OP104 No 30 38 35 Male Male\r## 5 OP105 Yes 41 32 37 Male Female\r## 6 OP106 Yes 41 37 38 Male Female\r## PC_reltoTC PC_marital_status PC_partner_inhome TC_race_Wh TC_race_Bl\r## 1 Bio Mom Married Yes 1 0\r## 2 Bio Mom Single No 1 0\r## 3 Bio Mom Married Yes 1 0\r## 4 Bio Dad Living together Yes 1 0\r## 5 Bio Mom Married Yes 1 0\r## 6 Foster Mom Married Yes 0 0\r## TC_race_His TC_race_As TC_race_Nat TC_race_Pac TC_race_Oth\r## 1 0 0 0 0 0\r## 2 0 0 0 0 0\r## 3 0 0 0 0 0\r## 4 0 0 0 0 0\r## 5 0 0 0 0 0\r## 6 0 0 1 0 0\r## PC_yrs_ed AC_yrs_ed\r## 1 HS grad-GED Specialized training\r## 2 HS grad-GED \u0026lt;NA\u0026gt;\r## 3 Partial high school Partial high school\r## 4 Partial college Partial college\r## 5 Partial college Partial college\r## 6 Standard college-university graduate Junior college-Associates degree\r## PC_employment PC_employ_hours AC_employment PC_annual_income\r## 1 Full time homemaker 0 Full time $80,000 to $89,999\r## 2 Unemployed 0 \u0026lt;NA\u0026gt; $5,000 to $9,999\r## 3 Full time homemaker 0 Full time $20,000 to $24,999\r## 4 Part time 24 Full time homemaker $10,000 to $14,999\r## 5 Full time homemaker 0 Part time $40,000 to $49,999\r## 6 Part time 20 Full time $80,000 to $89,999\r## gov_assist PC_fam_learning PC_fam_mentalh TC_diagnosis\r## 1 1 No No ASD\r## 2 1 No Yes Speech/Language Delay\r## 3 1 No Yes Social-Emotional Disorder\r## 4 1 Yes No ASD\r## 5 1 No No Speech/Language Delay\r## 6 1 Yes No Speech/Language Delay\r## TC_diagnosis_age TC_SPED TC_other_services dyad_adjust\r## 1 Two-years old (24-35 months) Yes Yes 104\r## 2 One-year old (12-23 months) Yes Yes 102\r## 3 Three-years old (36-47 months) Yes Yes 112\r## 4 One-year old (12-23 months) Yes Yes 98\r## 5 Three-years old (36-47 months) Yes Yes 84\r## 6 Two-years old (24-35 months) Yes Yes 139\r## fam_support informal_support formal_support CES_D CBCL_int_r CBCL_ext_r\r## 1 3 13 13 30 29 19\r## 2 8 6 20 19 32 46\r## 3 8 11 20 38 26 25\r## 4 9 8 25 17 21 17\r## 5 11 8 6 22 22 39\r## 6 23 23 23 1 25 28\r## VABS_comm_ss VABS_dls_ss VABS_soc_ss VABS_mot_ss PCBOS_Aggr_CU\r## 1 59 64 65 81 0\r## 2 69 78 80 69 0\r## 3 65 75 66 78 0\r## 4 66 87 68 100 0\r## 5 74 83 61 88 0\r## 6 69 81 85 91 0\r## PCBOS_Aggr_Play PCBOS_Aggr_SA PCBOS_Crit_CU PCBOS_Crit_Play PCBOS_Crit_SA\r## 1 0 0 0 0 0\r## 2 0 0 0 0 2\r## 3 0 0 0 0 0\r## 4 0 0 0 0 0\r## 5 0 0 0 0 4\r## 6 0 0 0 0 0\r## PCBOS_CU_Comm PCBOS_CU_Comp PCBOS_DescComm_CU PCBOS_DescComm_Play\r## 1 8 0 1 1\r## 2 31 2 0 5\r## 3 8 8 0 6\r## 4 7 0 0 2\r## 5 16 12 0 3\r## 6 1 1 1 7\r## PCBOS_DescComm_SA PCBOS_Disr_CU PCBOS_Disr_Play PCBOS_Disr_SA PCBOS_InComm_CU\r## 1 0 0 0 0 2\r## 2 0 0 3 0 4\r## 3 4 0 0 0 3\r## 4 3 0 0 0 1\r## 5 3 0 0 0 3\r## 6 2 0 0 0 0\r## PCBOS_InComm_Play PCBOS_InComm_SA PCBOS_Intr_CU PCBOS_Intr_Play PCBOS_Intr_SA\r## 1 7 4 1 4 2\r## 2 9 5 0 3 3\r## 3 10 1 0 0 1\r## 4 7 4 0 0 0\r## 5 4 0 1 1 5\r## 6 3 1 0 2 0\r## PCBOS_IP_CU PCBOS_IP_Play PCBOS_IP_SA PCBOS_LackFT_CU PCBOS_LackFT_Play\r## 1 0 15 5 1 10\r## 2 0 8 3 4 7\r## 3 0 12 2 3 1\r## 4 0 4 3 0 0\r## 5 0 4 1 3 0\r## 6 0 6 4 0 0\r## PCBOS_LackFT_SA PCBOS_NegV_CU PCBOS_NegV_Play PCBOS_NegV_SA PCBOS_PhysAgg_CU\r## 1 2 3 14 6 0\r## 2 4 4 0 0 0\r## 3 0 0 0 0 0\r## 4 1 3 0 0 0\r## 5 0 1 1 0 0\r## 6 1 0 0 0 0\r## PCBOS_PhysAgg_Play PCBOS_PhysAgg_SA PCBOS_PosConsq_CU PCBOS_PosConsq_Play\r## 1 0 0 0 0\r## 2 0 0 4 2\r## 3 0 0 0 0\r## 4 0 0 0 0\r## 5 0 0 0 0\r## 6 0 0 0 0\r## PCBOS_PosConsq_SA PCBOS_PosV_CU PCBOS_PosV_Play PCBOS_PosV_SA PCBOS_Praise_CU\r## 1 0 0 4 0 2\r## 2 0 3 17 2 3\r## 3 0 0 0 0 1\r## 4 0 0 0 0 0\r## 5 0 3 17 5 1\r## 6 0 4 19 6 3\r## PCBOS_Praise_Play PCBOS_Praise_SA TC_sib_age_mean TC_sib_behavior_ct\r## 1 3 0 7.000000 0\r## 2 6 8 4.666667 2\r## 3 3 5 7.000000 1\r## 4 7 3 15.000000 0\r## 5 2 5 8.500000 0\r## 6 11 2 9.000000 3\r## TC_sib_learning_ct TC_sib_mentalh_ct TC_sib_n_ct TC_sib_yes_ct\r## 1 1 0 1 1\r## 2 2 0 3 1\r## 3 0 1 2 1\r## 4 1 0 1 1\r## 5 1 0 2 1\r## 6 3 4 7 1\rVisualizing Missing Data\rlook \u0026lt;- ff_glimpse(og_data)\rlook$Continuous %\u0026gt;% arrange(missing_n)\r## label var_type n missing_n missing_percent\r## TC_age_mos TC_age_mos \u0026lt;int\u0026gt; 358 1 0.3\r## TC_race_Wh TC_race_Wh \u0026lt;int\u0026gt; 358 1 0.3\r## TC_race_Bl TC_race_Bl \u0026lt;int\u0026gt; 358 1 0.3\r## TC_race_His TC_race_His \u0026lt;int\u0026gt; 358 1 0.3\r## TC_race_As TC_race_As \u0026lt;int\u0026gt; 358 1 0.3\r## TC_race_Nat TC_race_Nat \u0026lt;int\u0026gt; 358 1 0.3\r## TC_race_Pac TC_race_Pac \u0026lt;int\u0026gt; 358 1 0.3\r## TC_race_Oth TC_race_Oth \u0026lt;int\u0026gt; 358 1 0.3\r## PC_employ_hours PC_employ_hours \u0026lt;int\u0026gt; 358 1 0.3\r## gov_assist gov_assist \u0026lt;int\u0026gt; 358 1 0.3\r## TC_sib_yes_ct TC_sib_yes_ct \u0026lt;int\u0026gt; 358 1 0.3\r## CBCL_int_r CBCL_int_r \u0026lt;int\u0026gt; 352 7 1.9\r## CBCL_ext_r CBCL_ext_r \u0026lt;int\u0026gt; 352 7 1.9\r## PC_age PC_age \u0026lt;int\u0026gt; 350 9 2.5\r## PCBOS_Aggr_CU PCBOS_Aggr_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Aggr_Play PCBOS_Aggr_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Aggr_SA PCBOS_Aggr_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Crit_CU PCBOS_Crit_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Crit_Play PCBOS_Crit_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Crit_SA PCBOS_Crit_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_CU_Comm PCBOS_CU_Comm \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_CU_Comp PCBOS_CU_Comp \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_DescComm_CU PCBOS_DescComm_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_DescComm_Play PCBOS_DescComm_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_DescComm_SA PCBOS_DescComm_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Disr_CU PCBOS_Disr_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Disr_Play PCBOS_Disr_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Disr_SA PCBOS_Disr_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_InComm_CU PCBOS_InComm_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_InComm_Play PCBOS_InComm_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_InComm_SA PCBOS_InComm_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Intr_CU PCBOS_Intr_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Intr_Play PCBOS_Intr_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Intr_SA PCBOS_Intr_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_IP_CU PCBOS_IP_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_IP_Play PCBOS_IP_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_IP_SA PCBOS_IP_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_LackFT_CU PCBOS_LackFT_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_LackFT_Play PCBOS_LackFT_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_LackFT_SA PCBOS_LackFT_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_NegV_CU PCBOS_NegV_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_NegV_Play PCBOS_NegV_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_NegV_SA PCBOS_NegV_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PhysAgg_CU PCBOS_PhysAgg_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PhysAgg_Play PCBOS_PhysAgg_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PhysAgg_SA PCBOS_PhysAgg_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PosConsq_CU PCBOS_PosConsq_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PosConsq_Play PCBOS_PosConsq_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PosConsq_SA PCBOS_PosConsq_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PosV_CU PCBOS_PosV_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PosV_Play PCBOS_PosV_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_PosV_SA PCBOS_PosV_SA \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Praise_CU PCBOS_Praise_CU \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Praise_Play PCBOS_Praise_Play \u0026lt;int\u0026gt; 344 15 4.2\r## PCBOS_Praise_SA PCBOS_Praise_SA \u0026lt;int\u0026gt; 344 15 4.2\r## fam_support fam_support \u0026lt;int\u0026gt; 334 25 7.0\r## informal_support informal_support \u0026lt;int\u0026gt; 334 25 7.0\r## formal_support formal_support \u0026lt;int\u0026gt; 334 25 7.0\r## CES_D CES_D \u0026lt;dbl\u0026gt; 334 25 7.0\r## AC_age AC_age \u0026lt;int\u0026gt; 297 62 17.3\r## VABS_comm_ss VABS_comm_ss \u0026lt;int\u0026gt; 285 74 20.6\r## VABS_dls_ss VABS_dls_ss \u0026lt;int\u0026gt; 285 74 20.6\r## VABS_soc_ss VABS_soc_ss \u0026lt;int\u0026gt; 285 74 20.6\r## dyad_adjust dyad_adjust \u0026lt;int\u0026gt; 279 80 22.3\r## TC_sib_behavior_ct TC_sib_behavior_ct \u0026lt;int\u0026gt; 279 80 22.3\r## TC_sib_learning_ct TC_sib_learning_ct \u0026lt;int\u0026gt; 279 80 22.3\r## TC_sib_mentalh_ct TC_sib_mentalh_ct \u0026lt;int\u0026gt; 279 80 22.3\r## TC_sib_n_ct TC_sib_n_ct \u0026lt;int\u0026gt; 279 80 22.3\r## TC_sib_age_mean TC_sib_age_mean \u0026lt;dbl\u0026gt; 278 81 22.6\r## VABS_mot_ss VABS_mot_ss \u0026lt;int\u0026gt; 273 86 24.0\r## mean sd min quartile_25 median quartile_75 max\r## TC_age_mos 43.4 10.5 24.0 36.0 41.0 48.0 71.0\r## TC_race_Wh 0.7 0.5 0.0 0.0 1.0 1.0 1.0\r## TC_race_Bl 0.1 0.3 0.0 0.0 0.0 0.0 1.0\r## TC_race_His 0.4 0.5 0.0 0.0 0.0 1.0 1.0\r## TC_race_As 0.0 0.2 0.0 0.0 0.0 0.0 1.0\r## TC_race_Nat 0.1 0.2 0.0 0.0 0.0 0.0 1.0\r## TC_race_Pac 0.0 0.1 0.0 0.0 0.0 0.0 1.0\r## TC_race_Oth 0.0 0.2 0.0 0.0 0.0 0.0 1.0\r## PC_employ_hours 21.8 26.6 0.0 0.0 11.0 40.0 99.0\r## gov_assist 0.6 0.5 0.0 0.0 1.0 1.0 1.0\r## TC_sib_yes_ct 0.8 0.4 0.0 1.0 1.0 1.0 1.0\r## CBCL_int_r 36.8 25.7 0.0 11.0 36.0 59.0 100.0\r## CBCL_ext_r 42.0 25.0 0.0 19.0 42.5 64.0 100.0\r## PC_age 34.6 7.9 19.0 29.0 34.0 39.0 67.0\r## PCBOS_Aggr_CU 0.0 0.3 0.0 0.0 0.0 0.0 3.0\r## PCBOS_Aggr_Play 0.0 0.3 0.0 0.0 0.0 0.0 3.0\r## PCBOS_Aggr_SA 0.0 0.2 0.0 0.0 0.0 0.0 2.0\r## PCBOS_Crit_CU 0.0 0.2 0.0 0.0 0.0 0.0 3.0\r## PCBOS_Crit_Play 0.1 0.4 0.0 0.0 0.0 0.0 4.0\r## PCBOS_Crit_SA 0.1 0.4 0.0 0.0 0.0 0.0 4.0\r## PCBOS_CU_Comm 20.1 12.8 0.0 11.0 18.0 27.0 68.0\r## PCBOS_CU_Comp 5.2 4.0 0.0 2.0 5.0 8.0 21.0\r## PCBOS_DescComm_CU 0.3 0.7 0.0 0.0 0.0 0.2 4.0\r## PCBOS_DescComm_Play 6.3 3.8 0.0 3.0 6.0 9.0 18.0\r## PCBOS_DescComm_SA 1.9 1.5 0.0 1.0 2.0 3.0 6.0\r## PCBOS_Disr_CU 0.2 0.6 0.0 0.0 0.0 0.0 4.0\r## PCBOS_Disr_Play 0.3 1.2 0.0 0.0 0.0 0.0 13.0\r## PCBOS_Disr_SA 0.1 0.6 0.0 0.0 0.0 0.0 5.0\r## PCBOS_InComm_CU 3.1 1.0 0.0 3.0 3.0 4.0 4.0\r## PCBOS_InComm_Play 6.7 4.5 0.0 3.0 6.0 9.2 19.0\r## PCBOS_InComm_SA 2.9 1.8 0.0 2.0 3.0 4.0 6.0\r## PCBOS_Intr_CU 0.5 0.9 0.0 0.0 0.0 1.0 4.0\r## PCBOS_Intr_Play 5.6 4.0 0.0 3.0 5.0 8.0 20.0\r## PCBOS_Intr_SA 3.0 1.8 0.0 2.0 3.0 5.0 6.0\r## PCBOS_IP_CU 0.1 0.3 0.0 0.0 0.0 0.0 3.0\r## PCBOS_IP_Play 3.8 3.3 0.0 1.0 3.0 6.0 19.0\r## PCBOS_IP_SA 1.5 1.4 0.0 0.0 1.0 2.0 6.0\r## PCBOS_LackFT_CU 2.9 1.2 0.0 2.0 3.0 4.0 4.0\r## PCBOS_LackFT_Play 6.7 4.7 0.0 3.0 6.0 10.0 19.0\r## PCBOS_LackFT_SA 2.8 1.9 0.0 1.0 3.0 5.0 6.0\r## PCBOS_NegV_CU 1.1 1.4 0.0 0.0 0.0 2.0 4.0\r## PCBOS_NegV_Play 1.1 2.7 0.0 0.0 0.0 1.0 20.0\r## PCBOS_NegV_SA 0.8 1.5 0.0 0.0 0.0 1.0 6.0\r## PCBOS_PhysAgg_CU 0.0 0.1 0.0 0.0 0.0 0.0 2.0\r## PCBOS_PhysAgg_Play 0.0 0.2 0.0 0.0 0.0 0.0 2.0\r## PCBOS_PhysAgg_SA 0.0 0.2 0.0 0.0 0.0 0.0 2.0\r## PCBOS_PosConsq_CU 0.2 0.7 0.0 0.0 0.0 0.0 4.0\r## PCBOS_PosConsq_Play 0.2 0.7 0.0 0.0 0.0 0.0 7.0\r## PCBOS_PosConsq_SA 0.0 0.3 0.0 0.0 0.0 0.0 4.0\r## PCBOS_PosV_CU 2.3 1.6 0.0 1.0 3.0 4.0 4.0\r## PCBOS_PosV_Play 14.7 6.7 0.0 12.0 18.0 20.0 20.0\r## PCBOS_PosV_SA 4.2 2.1 0.0 3.0 5.0 6.0 6.0\r## PCBOS_Praise_CU 2.8 3.2 0.0 0.0 2.0 4.0 15.0\r## PCBOS_Praise_Play 6.5 6.5 0.0 2.0 5.0 9.0 41.0\r## PCBOS_Praise_SA 6.2 4.6 0.0 2.0 5.0 9.0 24.0\r## fam_support 12.8 6.1 0.0 9.0 12.0 17.0 30.0\r## informal_support 7.3 5.6 0.0 3.0 6.0 10.0 26.0\r## formal_support 13.9 6.4 0.0 9.0 13.0 19.0 30.0\r## CES_D 13.4 10.4 0.0 6.0 10.5 18.0 48.0\r## AC_age 37.6 8.7 21.0 31.0 36.0 42.0 67.0\r## VABS_comm_ss 75.8 13.8 17.0 67.0 76.0 84.0 143.0\r## VABS_dls_ss 81.2 14.9 35.0 71.0 81.0 91.0 143.0\r## VABS_soc_ss 80.2 12.6 40.0 74.0 79.0 88.0 152.0\r## dyad_adjust 108.4 24.2 5.0 97.0 113.0 125.0 151.0\r## TC_sib_behavior_ct 0.4 0.7 0.0 0.0 0.0 1.0 3.0\r## TC_sib_learning_ct 0.3 0.6 0.0 0.0 0.0 0.0 3.0\r## TC_sib_mentalh_ct 0.3 0.6 0.0 0.0 0.0 0.0 4.0\r## TC_sib_n_ct 1.8 1.2 1.0 1.0 1.0 2.0 7.0\r## TC_sib_age_mean 6.9 4.9 0.0 3.0 6.0 10.0 26.0\r## VABS_mot_ss 83.3 14.4 37.0 73.0 82.0 93.0 144.0\rlook$Categorical %\u0026gt;% arrange(missing_n)\r## label var_type n missing_n missing_percent\r## study_id study_id \u0026lt;chr\u0026gt; 359 0 0.0\r## PSI_total_clinical PSI_total_clinical \u0026lt;chr\u0026gt; 359 0 0.0\r## TC_gender TC_gender \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_gender PC_gender \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_reltoTC PC_reltoTC \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_marital_status PC_marital_status \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_partner_inhome PC_partner_inhome \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_yrs_ed PC_yrs_ed \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_employment PC_employment \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_fam_learning PC_fam_learning \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_fam_mentalh PC_fam_mentalh \u0026lt;chr\u0026gt; 358 1 0.3\r## PC_annual_income PC_annual_income \u0026lt;chr\u0026gt; 357 2 0.6\r## TC_other_services TC_other_services \u0026lt;chr\u0026gt; 357 2 0.6\r## TC_diagnosis TC_diagnosis \u0026lt;chr\u0026gt; 356 3 0.8\r## TC_SPED TC_SPED \u0026lt;chr\u0026gt; 350 9 2.5\r## TC_diagnosis_age TC_diagnosis_age \u0026lt;chr\u0026gt; 346 13 3.6\r## AC_yrs_ed AC_yrs_ed \u0026lt;chr\u0026gt; 301 58 16.2\r## AC_employment AC_employment \u0026lt;chr\u0026gt; 300 59 16.4\r## levels_n levels levels_count levels_percent\r## study_id 359 - - -\r## PSI_total_clinical 2 - - -\r## TC_gender 2 - - -\r## PC_gender 2 - - -\r## PC_reltoTC 9 - - -\r## PC_marital_status 6 - - -\r## PC_partner_inhome 2 - - -\r## PC_yrs_ed 16 - - -\r## PC_employment 10 - - -\r## PC_fam_learning 2 - - -\r## PC_fam_mentalh 2 - - -\r## PC_annual_income 13 - - -\r## TC_other_services 2 - - -\r## TC_diagnosis 15 - - -\r## TC_SPED 4 - - -\r## TC_diagnosis_age 6 - - -\r## AC_yrs_ed 15 - - -\r## AC_employment 9 - - -\r#Examine levels for categorical data\rlook$Categorical %\u0026gt;% select(label, n, levels_n)\r## label n levels_n\r## study_id study_id 359 359\r## PSI_total_clinical PSI_total_clinical 359 2\r## TC_gender TC_gender 358 2\r## PC_gender PC_gender 358 2\r## PC_reltoTC PC_reltoTC 358 9\r## PC_marital_status PC_marital_status 358 6\r## PC_partner_inhome PC_partner_inhome 358 2\r## PC_yrs_ed PC_yrs_ed 358 16\r## AC_yrs_ed AC_yrs_ed 301 15\r## PC_employment PC_employment 358 10\r## AC_employment AC_employment 300 9\r## PC_annual_income PC_annual_income 357 13\r## PC_fam_learning PC_fam_learning 358 2\r## PC_fam_mentalh PC_fam_mentalh 358 2\r## TC_diagnosis TC_diagnosis 356 15\r## TC_diagnosis_age TC_diagnosis_age 346 6\r## TC_SPED TC_SPED 350 4\r## TC_other_services TC_other_services 357 2\r#look @ distributions of numeric data\rcont_labels \u0026lt;- look$Continuous$label\rhist.data.frame(og_data %\u0026gt;% select(all_of(cont_labels)))\rPreprocessing Steps - 1) Get rid of rows with no outcome data (moved out of blueprint for error testing) 2) Missing variable column for - AC Age, TC_sib (NA is meaningful) 3) Make yrs ed into numeric variable Make income into numeric variable Make age @ dx into numeric variable 4) For factor variables w/ high number of levels, make “other” 5) Dummy code categorical variables, and drop OG 6) Multiple imputation is reasonable for most variables 7) For variables where NA is meaningful (= absence of AC or sib, set remaining values to 0 for regression models)\nMaking a Blueprint\routcome \u0026lt;- \u0026quot;PSI_total_clinical\u0026quot;\rcat2num \u0026lt;- names(select(og_data, matches(\u0026quot;yrs_ed|diagnosis_age|income\u0026quot;))) #variables that will be recoded to be numeric\rcategorical \u0026lt;- names(select(og_data, where(is.character) \u0026amp; !(matches(\u0026quot;id|PSI\u0026quot;))))\rcategorical \u0026lt;- categorical[!categorical %in% cat2num]\rnumeric \u0026lt;- c(names(select(og_data, where(is.numeric))), cat2num)\ralready_dum \u0026lt;- names(select(og_data, matches(\u0026quot;TC_race|gov_assist|TC_sib_yes_ct\u0026quot;))) #some variables already \u0026quot;dummy\u0026quot; coded\rtrue_numeric \u0026lt;- numeric[!numeric %in% already_dum]\rna_true \u0026lt;- names(select(og_data, matches(\u0026quot;AC|TC_sib\u0026quot;, ignore.case = FALSE))) #don\u0026#39;t impute data for alternate caregiver or subling variables\rimpute_vars \u0026lt;- true_numeric[!true_numeric %in% na_true]\rstress_blueprint \u0026lt;- recipe(x = og_data, vars = names(og_data),\rroles = c(\u0026quot;ID\u0026quot;, \u0026quot;outcome\u0026quot;, rep(\u0026quot;predictor\u0026quot;, 86))) %\u0026gt;% #step_filter(!is.na(PSI_total_clinical), skip = FALSE) %\u0026gt;% #causing error step_indicate_na(matches(\u0026quot;AC_age\u0026quot;, \u0026quot;TC_sib_yes_ct\u0026quot;)) %\u0026gt;%\rstep_other(matches(\u0026quot;PC_reltoTC|employment|TC_diagnosis|PC_marital_status|TC_SPED\u0026quot;)) %\u0026gt;%\r#making yrs ed numeric\rstep_mutate(across(matches(\u0026quot;yrs_ed\u0026quot;), ~case_when(\rstr_detect(., \u0026quot;No formal\u0026quot;) ~ 0,\rstr_detect(., \u0026quot;7th grade\u0026quot;) ~ 7, str_detect(., \u0026quot;Junior high\u0026quot;) ~ 8,\rstr_detect(., \u0026quot;Partial high school\u0026quot;) ~ 9,\rstr_detect(., \u0026quot;GED\u0026quot;) ~ 12,\rstr_detect(., \u0026quot;Partial college\u0026quot;) ~ 13,\rstr_detect(., \u0026quot;Specialized\u0026quot;) ~ 14,\rstr_detect(., \u0026quot;Associates\u0026quot;) ~ 14,\rstr_detect(., \u0026quot;university grad\u0026quot;) ~ 16,\rstr_detect(., \u0026quot;Grad prof\u0026quot;) ~ 18))) %\u0026gt;%\r#making dx age numeric\rstep_mutate(across(matches(\u0026quot;diagnosis_age\u0026quot;), ~case_when(\rstr_detect(., \u0026quot;At birth\u0026quot;) ~ 6,\rstr_detect(., \u0026quot;One\u0026quot;) ~ 18, str_detect(., \u0026quot;Two\u0026quot;) ~ 30,\rstr_detect(., \u0026quot;Three\u0026quot;) ~ 42,\rstr_detect(., \u0026quot;Four\u0026quot;) ~ 54,\rstr_detect(., \u0026quot;Five\u0026quot;) ~ 66))) %\u0026gt;%\r#making annual income numeric\rstep_mutate(PC_annual_income = ifelse(str_detect(PC_annual_income, \u0026quot;less\u0026quot;), 2500,\rifelse(str_detect(PC_annual_income, \u0026quot;14,999\u0026quot;), 12500,\rifelse(str_detect(PC_annual_income, \u0026quot;19,999\u0026quot;), 17500,\rifelse(str_detect(PC_annual_income, \u0026quot;24,999\u0026quot;), 22500,\rifelse(str_detect(PC_annual_income, \u0026quot;29,999\u0026quot;), 27500,\rifelse(str_detect(PC_annual_income, \u0026quot;39,999\u0026quot;), 35000,\rifelse(str_detect(PC_annual_income, \u0026quot;49,999\u0026quot;), 45000,\rifelse(str_detect(PC_annual_income, \u0026quot;59,999\u0026quot;), 55000,\rifelse(str_detect(PC_annual_income, \u0026quot;69,999\u0026quot;), 65000,\rifelse(str_detect(PC_annual_income, \u0026quot;79,999\u0026quot;), 75000,\rifelse(str_detect(PC_annual_income, \u0026quot;89,999\u0026quot;), 85000,\rifelse(str_detect(PC_annual_income, \u0026quot;more\u0026quot;), 95000,\rifelse(is.na(PC_annual_income), NA, 7500)))))))))))))) %\u0026gt;% #5,000 and $9,999 in all others\rstep_impute_mode(all_of(categorical)) %\u0026gt;%\rstep_zv(all_of(numeric)) %\u0026gt;%\rstep_dummy(all_of(categorical)) %\u0026gt;%\rstep_mutate(across(matches(\u0026quot;AC|TC_sib\u0026quot;, ignore.case = FALSE), ~replace_na(., 0))) %\u0026gt;%\rstep_impute_knn(all_of(impute_vars), all_of(already_dum)) %\u0026gt;%\rstep_normalize(all_of(true_numeric))\rprepare \u0026lt;- prep(stress_blueprint, training = og_data)\rstress_data \u0026lt;- bake(prepare, new_data = og_data)\rhead(stress_data)\r## # A tibble: 6 x 101\r## study_id PSI_total_clinical TC_age_mos PC_age AC_age TC_race_Wh TC_race_Bl\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 OP101 Yes -0.131 0.303 0.548 1 0\r## 2 OP102 Yes -1.08 -0.968 -1.91 1 0\r## 3 OP103 Yes -0.131 -0.587 -0.127 1 0\r## 4 OP104 No -1.27 0.430 0.241 1 0\r## 5 OP105 Yes -0.226 -0.332 0.364 1 0\r## 6 OP106 Yes -0.226 0.303 0.425 0 0\r## # ... with 94 more variables: TC_race_His \u0026lt;int\u0026gt;, TC_race_As \u0026lt;int\u0026gt;,\r## # TC_race_Nat \u0026lt;int\u0026gt;, TC_race_Pac \u0026lt;int\u0026gt;, TC_race_Oth \u0026lt;int\u0026gt;, PC_yrs_ed \u0026lt;dbl\u0026gt;,\r## # AC_yrs_ed \u0026lt;dbl\u0026gt;, PC_employ_hours \u0026lt;dbl\u0026gt;, PC_annual_income \u0026lt;dbl\u0026gt;,\r## # gov_assist \u0026lt;int\u0026gt;, TC_diagnosis_age \u0026lt;dbl\u0026gt;, dyad_adjust \u0026lt;dbl\u0026gt;,\r## # fam_support \u0026lt;dbl\u0026gt;, informal_support \u0026lt;dbl\u0026gt;, formal_support \u0026lt;dbl\u0026gt;,\r## # CES_D \u0026lt;dbl\u0026gt;, CBCL_int_r \u0026lt;dbl\u0026gt;, CBCL_ext_r \u0026lt;dbl\u0026gt;, VABS_comm_ss \u0026lt;dbl\u0026gt;,\r## # VABS_dls_ss \u0026lt;dbl\u0026gt;, VABS_soc_ss \u0026lt;dbl\u0026gt;, VABS_mot_ss \u0026lt;dbl\u0026gt;, ...\r#Check for missingness\rlook2 \u0026lt;- ff_glimpse(stress_data)\rlook2$Continuous %\u0026gt;% arrange(desc(missing_n)) %\u0026gt;% filter(missing_n \u0026gt; 0)\r## [1] label var_type n missing_n ## [5] missing_percent mean sd min ## [9] quartile_25 median quartile_75 max ## \u0026lt;0 rows\u0026gt; (or 0-length row.names)\rlook2$Categorical %\u0026gt;% arrange(missing_n) %\u0026gt;% filter(missing_n \u0026gt; 0)\r## [1] label var_type n missing_n ## [5] missing_percent levels_n levels levels_count ## [9] levels_percent ## \u0026lt;0 rows\u0026gt; (or 0-length row.names)\rTest and Training Set\rset.seed(121221)\rsplit \u0026lt;- initial_split(og_data, prop = .75)\rstress_train \u0026lt;- training(split)\rstress_test \u0026lt;- testing(split)\rprepare \u0026lt;- prep(stress_blueprint, training = stress_train)\rbaked_train \u0026lt;- bake(prepare, new_data = stress_train)\rbaked_test \u0026lt;- bake(prepare, new_data = stress_test)\rFunctions\r##Making a crossfold function\rcrossfold_log \u0026lt;- function(training_data, folds){\r#shuffle data\rtraning_data \u0026lt;- training_data[sample(nrow(training_data)),]\r# Create 10 folds with equal size\rN_folds = cut(seq(1,nrow(training_data)),breaks= folds,labels=FALSE)\r# Create the list for each fold my.indices \u0026lt;- vector(\u0026#39;list\u0026#39;,folds)\rfor(i in 1:folds){\rmy.indices[[i]] \u0026lt;- which(N_folds!=i)\r}\r#cross validation settings\rcv \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;,\rindex = my.indices,\rclassProbs = TRUE,\rsummaryFunction = mnLogLoss)\rreturn(cv)\r}\r#Making an accuracy function accuracy \u0026lt;- function(observed_vector, predicted_vector){\rtab \u0026lt;- table(predicted_vector,\robserved_vector,\rdnn = c(\u0026#39;Predicted\u0026#39;,\u0026#39;Observed\u0026#39;))\rtn \u0026lt;- tab[1,1]\rtp \u0026lt;- tab[2,2]\rfp \u0026lt;- tab[2,1]\rfn \u0026lt;- tab[1,2]\racc \u0026lt;- (tp + tn)/(tp+tn+fp+fn)\rreturn(acc)\r}\rModel Building\rModel 1\rRidge Regression\ncf \u0026lt;- crossfold_log(stress_train, 10) #error occurs for both stress_train and baked_train\rfolds \u0026lt;- vfold_cv(stress_train, 10)\rcv \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;,\rnumber = 10,\rclassProbs = TRUE,\rsummaryFunction = mnLogLoss)\rridge_grid \u0026lt;- data.frame(alpha = 0, lambda = c(seq(.01, 5, .1)))\rmod_1 \u0026lt;- caret::train(stress_blueprint,\rdata = stress_train, method = \u0026quot;glmnet\u0026quot;,\rfamily = \u0026#39;binomial\u0026#39;,\rmetric = \u0026#39;logLoss\u0026#39;,\rtrControl = cf,\rtuneGrid = ridge_grid)\rmod_1$bestTune\rplot(mod_1)\rI get an error - “number of rows of result is not a multiple of vector length (arg 1)”… It’s possible rows are not lining up correctly either w/ the blueprint or cross validation settings. blueprint seems to be working to “bake” data… This goes away when filtering step is applied outside of blueprint\nWhen filter step applied prior to blueprint, get errors for applying blueprint within model..\nModel Eval Metrics and Variable Importance\rpredicted_test \u0026lt;- predict(mod_1, stress_test) %\u0026gt;% as.numeric() %\u0026gt;% -1 #subtracting 1 b/c factor levels come out as 1/2 rather than 0/1\robserved_test \u0026lt;- stress_test$PSI_total_clinical %\u0026gt;% as.numeric()\r#LogLoss ACC ROC Sensitivity Specificity PPV\r#LogLoss\rLL \u0026lt;- logLoss(observed_test, predicted_test)\r#AUC\rAUC \u0026lt;- auc(observed_test, predicted_test)\r#Accuracy\rACC \u0026lt;- accuracy(observed_test, predicted_test)\r#Sensitivity\rTPR \u0026lt;- tpr(observed_test, predicted_test, cutoff = .5)\r#Specificity\rTNR \u0026lt;- tnr(observed_test, predicted_test, cutoff = .5)\r#PPV\rPRE \u0026lt;- precision(observed_test, predicted_test, cutoff = .5)\rmod_1_stats \u0026lt;- c(\u0026quot;Ridge Regression\u0026quot;, LL, AUC, TPR, TNR, PRE)\r#Looking @ VIP for 20 feats\rvip(mod_1, num_features = 20, geom = \u0026quot;point\u0026quot;) + theme_bw()\rModel 2\rRandom Forests\ncf \u0026lt;- crossfold_log(stress_train, 10)\r#tune trees #Can do similar tuning for max depth\rmod_tune \u0026lt;- vector(\u0026#39;list\u0026#39;,200)\rfor(i in 1:200){\rmod_tune[[i]] \u0026lt;- caret::train(blueprint_recidivism,\rdata = recidivism_tr,\rmethod = \u0026#39;ranger\u0026#39;,\rtrControl = cv,\rtuneGrid = grid,\rmetric = \u0026#39;logLoss\u0026#39;,\rnum.trees = i,\rmax.depth = 60)\r}\rlogLoss_ \u0026lt;- c()\rfor(i in 1:200){\rlogLoss_[i] = mod_tune[[i]]$results$logLoss\r}\rggplot()+\rgeom_line(aes(x=1:200,y=logLoss_))+\rxlab(\u0026#39;Number of Tree Models\u0026#39;)+\rylab(\u0026#39;Negative LogLoss\u0026#39;)+\rylim(c(0,12))+\rtheme_bw()\rOnly tune N predictors\n# Grid settings\rgrid \u0026lt;- expand.grid(mtry = 30,\rsplitrule=\u0026#39;gini\u0026#39;,\rmin.node.size=2)\rmod_2 \u0026lt;- caret::train(stress_blueprint,\rdata = stress_train,\rmethod = \u0026#39;ranger\u0026#39;,\rtrControl = cf,\rtuneGrid = grid,\rnum.trees = 500,\rmax.depth = 10)\rWhen filter in blueprint - Similar error occurs here, x Existing data has 27 rows. x Assigned data has 28 rows.\nIt seems something is potentially going badly when making the cv setting - perhaps because blueprint uses some filter function?\nWith filter not in blueprint: Currently get errors – Warning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = n, : skipping variable with zero or non-finite range Warning: model fit failed for Resample09: mtry=30, splitrule=gini, min.node.size=2 Error : Can’t subset columns that don’t exist. x Column PCBOS_PhysAgg_SA doesn’t exist.\nError in { : task 1 failed - “$ operator is invalid for atomic vectors”\nModel 2 Performance\rpredicted_test \u0026lt;- predict(mod_2, stress_test) %\u0026gt;% as.numeric() %\u0026gt;% -1 #subtracting 1 b/c factor levels come out as 1/2 rather than 0/1\robserved_test \u0026lt;- stress_test$PSI_total_clinical %\u0026gt;% as.numeric()\r#LogLoss ACC ROC Sensitivity Specificity PPV\r#LogLoss\rLL \u0026lt;- logLoss(observed_test, predicted_test)\r#AUC\rAUC \u0026lt;- auc(observed_test, predicted_test)\r#Accuracy\rACC \u0026lt;- accuracy(observed_test, predicted_test)\r#Sensitivity\rTPR \u0026lt;- tpr(observed_test, predicted_test, cutoff = .5)\r#Specificity\rTNR \u0026lt;- tnr(observed_test, predicted_test, cutoff = .5)\r#PPV\rPRE \u0026lt;- precision(observed_test, predicted_test, cutoff = .5)\rmod_2_stats \u0026lt;- c(\u0026quot;Ridge Regression\u0026quot;, LL, AUC, TPR, TNR, PRE)\r#Looking @ VIP for 20 feats\rvip(mod_2, num_features = 20, geom = \u0026quot;point\u0026quot;) + theme_bw()\rCode Graveyard\r#step_mutate(PC_reltoTC = #ifelse(str_detect(PC_reltoTC, \u0026quot;Grandma|Grandpa|Aunt|Uncle\u0026quot;), \u0026quot;Kinship\u0026quot;, #ifelse(str_detect(PC_reltoTC, \u0026quot;Bio\u0026quot;), \u0026quot;Bio Parent\u0026quot;, #ifelse(str_detect(PC_reltoTC, \u0026quot;Adoptive|Partner|Step\u0026quot;), \u0026quot;Adoptive\u0026quot;, #ifelse(str_detect(PC_reltoTC, \u0026quot;Foster\u0026quot;), \u0026quot;Foster\u0026quot;, NA)))))\r#taken out for now, but could re-implement if need to get more specific\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9a964dde14403c13533af292dbe78cff","permalink":"/code/ml-class/final-project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/code/ml-class/final-project/","section":"code","summary":"\u003c!DOCTYPE html\u003e\rML Final\rML Final\rLiz G\r12/9/2021\rSet Up\rPackages and Import\rrequire(caret)\r## Loading required package: caret\r## Loading required package: ggplot2\r## Loading required package: lattice\rrequire(recipes)\r## Loading required package: recipes\r## Loading required package: dplyr\r## ## Attaching package: \u0026#39;dplyr\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## filter, lag\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## intersect, setdiff, setequal, union\r## ## Attaching package: \u0026#39;recipes\u0026#39;\r## The following object is masked from \u0026#39;package:stats\u0026#39;:\r## ## step\rrequire(ranger)\r## Loading required package: ranger\rrequire(tidyverse)\r## Loading required package: tidyverse\r## -- Attaching packages --------------------------------------- tidyverse 1.","tags":null,"title":"","type":"code"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e HW1 HW1 Liz G 11/11/2021 Import Dependencies\nrequire(tidyverse) ## Loading required package: tidyverse ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.5 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.4 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() require(reticulate) ## Loading required package: reticulate #Part 1: Text Data\n##Task 1.1\nog_text \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/tweet_sub.csv\u0026quot;) head(og_text) #Task 1.2\ntext \u0026lt;- og_text %\u0026gt;% separate(time, into = c(\u0026quot;day\u0026quot;, \u0026quot;month\u0026quot;, \u0026quot;date\u0026quot;, \u0026quot;TOD\u0026quot;, \u0026quot;tz\u0026quot;, \u0026quot;year\u0026quot;), sep = \u0026quot; \u0026quot;, remove = FALSE) %\u0026gt;% mutate(hour = str_remove(TOD, \u0026quot;(?\u0026lt;=\\\\d{2}):\\\\d{2}:\\\\d{2}\u0026quot;)) %\u0026gt;% select(-(TOD:year)) %\u0026gt;% mutate(day = ifelse(day == \u0026quot;Sun\u0026quot;, 1, ifelse(day == \u0026quot;Mon\u0026quot;, 2, ifelse(day == \u0026quot;Tue\u0026quot;, 3, ifelse(day == \u0026quot;Wed\u0026quot;, 4, ifelse(day == \u0026quot;Thu\u0026quot;, 5, ifelse(day == \u0026quot;Fri\u0026quot;, 6, ifelse(day == \u0026quot;Sat\u0026quot;, 7, NA)))))))) %\u0026gt;% mutate(across(c(date, hour), as.numeric)) head(text) #Task 1.3\ntext \u0026lt;- text %\u0026gt;% mutate(sentiment = ifelse(sentiment == \u0026quot;Positive\u0026quot;, 1, ifelse(sentiment == \u0026quot;Negative\u0026quot;, 0, \u0026quot;NA\u0026quot;))) table(text$sentiment) ## ## 0 1 ## 744 756 #Task 1.4\nrequire(text) ## Loading required package: text ## Registered S3 method overwritten by \u0026#39;tune\u0026#39;: ## method from ## required_pkgs.model_spec parsnip ## \u001b[0;34mThis is text (version 0.9.13). ## \u001b[0m\u001b[0;32mText is new and still rapidly improving. ## Newer versions may have improved functions and updated defaults to reflect current understandings of the state-of-the-art. ## Please send us feedback based on your experience.\u001b[0m text_embed \u0026lt;- text$tweet %\u0026gt;% textEmbed( model = \u0026quot;roberta-base\u0026quot;, layers = 1) text \u0026lt;- cbind(text, text_embed$x) ##Task 1.5\ntext \u0026lt;- text %\u0026gt;% select(-c(\u0026quot;time\u0026quot;, \u0026quot;tweet\u0026quot;)) ##Task 1.6\nrequire(recipes) ## Loading required package: recipes ## ## Attaching package: \u0026#39;recipes\u0026#39; ## The following object is masked from \u0026#39;package:stringr\u0026#39;: ## ## fixed ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## step outcome \u0026lt;- \u0026quot;sentiment\u0026quot; cyclic \u0026lt;- c(\u0026quot;day\u0026quot;, \u0026quot;date\u0026quot;, \u0026quot;hour\u0026quot;) categorical \u0026lt;- \u0026quot;month\u0026quot; text_blueprint \u0026lt;- recipe(x = text, vars = names(text), roles = c(\u0026quot;outcome\u0026quot;, rep(\u0026#39;predictor\u0026#39;, 772))) %\u0026gt;% step_harmonic(day, frequency = 1, cycle_size = 7) %\u0026gt;% step_harmonic(date, frequency = 1, cycle_size = 31) %\u0026gt;% step_harmonic(hour, frequency = 1, cycle_size = 24) %\u0026gt;% step_dummy(month, one_hot = TRUE) %\u0026gt;% step_normalize(contains(\u0026quot;Dim\u0026quot;)) print(text_blueprint) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 772 ## ## Operations: ## ## Harmonic numeric variables for day ## Harmonic numeric variables for date ## Harmonic numeric variables for hour ## Dummy variables from month ## Centering and scaling for contains(\u0026quot;Dim\u0026quot;) ##Task 1.7\ntext_prep \u0026lt;- prep(text_blueprint, text) text_baked \u0026lt;- bake(text_prep, text) ##Task 1.8\ntext_baked \u0026lt;- text_baked %\u0026gt;% select(-(c(day, date, hour))) ##Task 1.9\nwrite.csv(text_baked, \u0026quot;tweet_data_preprocessed.csv\u0026quot;) #Part 2: Continuous/Categorical Data\nLoad Dependencies\nrequire(finalfit) ## Loading required package: finalfit ##Task 2.1\nog_data \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/oregon.csv\u0026quot;) ##Task 2.2\ndata \u0026lt;- og_data %\u0026gt;% mutate(month = str_extract(tst_dt, \u0026quot;\\\\d{1,2}(?=/\\\\d{1,2}/\\\\d{4})\u0026quot;) %\u0026gt;% as.numeric(), date = str_extract(tst_dt, \u0026quot;(?\u0026lt;=\\\\d{1,2}/)\\\\d{1,2}(?=/\\\\d{4})\u0026quot;) %\u0026gt;% as.numeric()) %\u0026gt;% select(-tst_dt) table(data$date) ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 7490 3149 3225 5876 2957 1638 4569 5567 6787 7930 6015 618 158 ## 14 15 16 17 18 19 20 21 22 23 24 25 26 ## 5284 9072 10228 12311 8452 912 623 8231 13765 14749 15568 9524 1507 ## 27 28 29 30 31 ## 863 1 6801 8494 7062 table(data$month) ## ## 2 3 4 5 6 ## 33 421 13037 161473 14462 ##Task 2.3\nlook \u0026lt;- ff_glimpse(data) data \u0026lt;- data %\u0026gt;% select(-ayp_lep) ##Task 2.4\ndata \u0026lt;- data %\u0026gt;% mutate(trgt_assist_fg = str_replace(trgt_assist_fg, \u0026quot;y\u0026quot;, \u0026quot;Y\u0026quot;)) require(Hmisc) ## Loading required package: Hmisc ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## ## Attaching package: \u0026#39;Hmisc\u0026#39; ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## src, summarize ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## format.pval, units cont_labels \u0026lt;- look$Continuous$label hist.data.frame(data %\u0026gt;% select(all_of(cont_labels))) boxplot(data$score) ##Task 2.5\ndata \u0026lt;- data %\u0026gt;% select(enrl_grd, month, date, sex:grp_rpt_schl_prfrm, score, id) names(data) ## [1] \u0026quot;enrl_grd\u0026quot; \u0026quot;month\u0026quot; \u0026quot;date\u0026quot; ## [4] \u0026quot;sex\u0026quot; \u0026quot;ethnic_cd\u0026quot; \u0026quot;tst_bnch\u0026quot; ## [7] \u0026quot;migrant_ed_fg\u0026quot; \u0026quot;ind_ed_fg\u0026quot; \u0026quot;sp_ed_fg\u0026quot; ## [10] \u0026quot;tag_ed_fg\u0026quot; \u0026quot;econ_dsvntg\u0026quot; \u0026quot;stay_in_dist\u0026quot; ## [13] \u0026quot;stay_in_schl\u0026quot; \u0026quot;dist_sped\u0026quot; \u0026quot;trgt_assist_fg\u0026quot; ## [16] \u0026quot;ayp_dist_partic\u0026quot; \u0026quot;ayp_schl_partic\u0026quot; \u0026quot;ayp_dist_prfrm\u0026quot; ## [19] \u0026quot;ayp_schl_prfrm\u0026quot; \u0026quot;rc_dist_partic\u0026quot; \u0026quot;rc_schl_partic\u0026quot; ## [22] \u0026quot;rc_dist_prfrm\u0026quot; \u0026quot;rc_schl_prfrm\u0026quot; \u0026quot;grp_rpt_dist_partic\u0026quot; ## [25] \u0026quot;grp_rpt_schl_partic\u0026quot; \u0026quot;grp_rpt_dist_prfrm\u0026quot; \u0026quot;grp_rpt_schl_prfrm\u0026quot; ## [28] \u0026quot;score\u0026quot; \u0026quot;id\u0026quot; outcome \u0026lt;- \u0026quot;score\u0026quot; ID \u0026lt;- \u0026quot;id\u0026quot; numeric \u0026lt;- \u0026quot;enrl_grd\u0026quot; cyclic \u0026lt;- c(\u0026quot;date\u0026quot;, \u0026quot;month\u0026quot;) categorical \u0026lt;- names(select(data, sex:grp_rpt_schl_prfrm)) %\u0026gt;% as.character() data_blueprint \u0026lt;- recipe(x = data, vars = names(data), roles = c(rep(\u0026#39;predictor\u0026#39;, 27), \u0026quot;outcome\u0026quot;, \u0026quot;ID\u0026quot;)) %\u0026gt;% step_indicate_na(all_of(numeric), all_of(cyclic), all_of(categorical)) %\u0026gt;% step_zv(all_of(numeric), contains(\u0026quot;na\u0026quot;)) %\u0026gt;% step_impute_mean(all_of(numeric)) %\u0026gt;% step_impute_mode(all_of(categorical)) %\u0026gt;% step_harmonic(date, frequency = 1, cycle_size = 31) %\u0026gt;% step_harmonic(month, frequency = 1, cycle_size = 12) %\u0026gt;% step_ns(all_of(numeric), deg_free = 3) %\u0026gt;% step_normalize(paste0(numeric,\u0026#39;_ns_1\u0026#39;), paste0(numeric,\u0026#39;_ns_2\u0026#39;), paste0(numeric,\u0026#39;_ns_3\u0026#39;)) %\u0026gt;% step_dummy(all_of(categorical), one_hot = TRUE) print(data_blueprint) ## Recipe ## ## Inputs: ## ## role #variables ## ID 1 ## outcome 1 ## predictor 27 ## ## Operations: ## ## Creating missing data variable indicators for all_of(numeric), all_of(cyclic), all_of(categor... ## Zero variance filter on all_of(numeric), contains(\u0026quot;na\u0026quot;) ## Mean Imputation for all_of(numeric) ## Mode Imputation for all_of(categorical) ## Harmonic numeric variables for date ## Harmonic numeric variables for month ## Natural Splines on all_of(numeric) ## Centering and scaling for paste0(numeric, \u0026quot;_ns_1\u0026quot;), paste0(numeric, \u0026quot;_ns_... ## Dummy variables from all_of(categorical) ##Task 2.6\ndata_prep \u0026lt;- prep(data_blueprint, training = data) data_baked \u0026lt;- bake(data_prep, new_data = data) ##Task 2.7\ndata_baked \u0026lt;- select(data_baked, -(month:date)) ##Task 2.8\nwrite.csv(data_baked, \u0026quot;oregon_testing_preprocessed.csv\u0026quot;) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d16e23e70bf8254b4137eadc9f651df6","permalink":"/code/ml-class/hw1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/code/ml-class/hw1/","section":"code","summary":"\u003c!DOCTYPE html\u003e HW1 HW1 Liz G 11/11/2021 Import Dependencies require(tidyverse) ## Loading required package: tidyverse ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.","tags":null,"title":"","type":"code"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e\rHW2\rHW2\rLiz G\r11/14/2021\rPart 1: Logistic Regression\rLoading and Prepping Data\r# Load the following packages needed for modeling in this assignment\rrequire(caret)\r## Loading required package: caret\r## Loading required package: ggplot2\r## Loading required package: lattice\rrequire(recipes)\r## Loading required package: recipes\r## Loading required package: dplyr\r## ## Attaching package: \u0026#39;dplyr\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## filter, lag\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## intersect, setdiff, setequal, union\r## ## Attaching package: \u0026#39;recipes\u0026#39;\r## The following object is masked from \u0026#39;package:stats\u0026#39;:\r## ## step\rrequire(finalfit)\r## Loading required package: finalfit\rrequire(glmnet)\r## Loading required package: glmnet\r## Loading required package: Matrix\r## Loaded glmnet 4.1-2\rrequire(ModelMetrics)\r## Loading required package: ModelMetrics\r## ## Attaching package: \u0026#39;ModelMetrics\u0026#39;\r## The following objects are masked from \u0026#39;package:caret\u0026#39;:\r## ## confusionMatrix, precision, recall, sensitivity, specificity\r## The following object is masked from \u0026#39;package:base\u0026#39;:\r## ## kappa\r# Import the tweet dataset with embeddings\rtweet \u0026lt;- read.csv(\u0026#39;https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/hw1_tweet_final.csv\u0026#39;,header=TRUE)\r# Recipe for the tweet dataset\rblueprint_tweet \u0026lt;- recipe(x = tweet,\rvars = colnames(tweet),\rroles = c(\u0026#39;outcome\u0026#39;,rep(\u0026#39;predictor\u0026#39;,772))) %\u0026gt;%\rstep_dummy(\u0026#39;month\u0026#39;,one_hot=TRUE) %\u0026gt;% step_harmonic(\u0026#39;day\u0026#39;,frequency=1,cycle_size=7, role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_harmonic(\u0026#39;date\u0026#39;,frequency=1,cycle_size=31,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_harmonic(\u0026#39;hour\u0026#39;,frequency=1,cycle_size=24,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_normalize(paste0(\u0026#39;Dim\u0026#39;,1:768)) %\u0026gt;%\rstep_normalize(c(\u0026#39;day_sin_1\u0026#39;,\u0026#39;day_cos_1\u0026#39;,\r\u0026#39;date_sin_1\u0026#39;,\u0026#39;date_cos_1\u0026#39;,\r\u0026#39;hour_sin_1\u0026#39;,\u0026#39;hour_cos_1\u0026#39;)) %\u0026gt;%\rstep_num2factor(sentiment,\rtransform = function(x) x + 1,\rlevels=c(\u0026#39;Negative\u0026#39;,\u0026#39;Positive\u0026#39;))\r# Notice that I explicitly specified role=predictor when using\r# step_harmonic function. This assures that the newly derived sin and cos\r# variables has a defined role.\r# You need to do this otherwise caret::train function breaks.\r# caret_train requires every variable in the recipe to have a role\r# You can run the following code and make sure every variable has a defined # role. If you want to experiment, remove the role=predictor argument\r# in the step_harmonic function, create the recipe again, and run the following\r# you will see that the new sin and cos variables have NA in the column role\r# and this breaks the caret::train function later.\r# Also, in the last line, we transform the outcome variable \u0026#39;sentiment\u0026#39; to # a factor with labels. # This seems necessary for fitting logistic regression via caret::train\rprint(blueprint_tweet %\u0026gt;% prep() %\u0026gt;% summary)\r## # A tibble: 781 x 4\r## variable type role source ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 sentiment nominal outcome original\r## 2 day numeric predictor original\r## 3 date numeric predictor original\r## 4 hour numeric predictor original\r## 5 Dim1 numeric predictor original\r## 6 Dim2 numeric predictor original\r## 7 Dim3 numeric predictor original\r## 8 Dim4 numeric predictor original\r## 9 Dim5 numeric predictor original\r## 10 Dim6 numeric predictor original\r## # ... with 771 more rows\rTask 1.1\rSplit the original data into two subsets: training and test. Let the training data have the 80% of cases and the test data have the 20% of the cases.\nset.seed(11142021) # for reproducibility\rloc \u0026lt;- sample(1:nrow(tweet), round(nrow(tweet) * 0.8))\rtweet_train \u0026lt;- tweet[loc, ]\rtweet_test \u0026lt;- tweet[-loc, ]\rprepare \u0026lt;- prep(blueprint_tweet, training = tweet_train)\rbaked_train \u0026lt;- bake(prepare, new_data = tweet_train)\rbaked_test \u0026lt;- bake(prepare, new_data = tweet_test)\rTask 1.2\rUse the caret::train() function to train a model with 10-fold cross-validation for predicting the probability of sentiment being positive using logistic regression without any regularization. Evaluate and report the performance of the model on the test dataset.\n##Making a crossfold function\rcrossfold_log \u0026lt;- function(training_data, folds){\r#shuffle data\rtraning_data \u0026lt;- training_data[sample(nrow(training_data)),]\r# Create 10 folds with equal size\rN_folds = cut(seq(1,nrow(training_data)),breaks= folds,labels=FALSE)\r# Create the list for each fold my.indices \u0026lt;- vector(\u0026#39;list\u0026#39;,folds)\rfor(i in 1:folds){\rmy.indices[[i]] \u0026lt;- which(N_folds!=i)\r}\r#cross validation settings\rcv \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;,\rindex = my.indices,\rclassProbs = TRUE,\rsummaryFunction = mnLogLoss)\rreturn(cv)\r}\r#Making an accuracy function accuracy \u0026lt;- function(observed_vector, predicted_vector){\rtab \u0026lt;- table(predicted_vector,\robserved_vector,\rdnn = c(\u0026#39;Predicted\u0026#39;,\u0026#39;Observed\u0026#39;))\rtn \u0026lt;- tab[1,1]\rtp \u0026lt;- tab[2,2]\rfp \u0026lt;- tab[2,1]\rfn \u0026lt;- tab[1,2]\racc \u0026lt;- (tp + tn)/(tp+tn+fp+fn)\rreturn(acc)\r}\rcrossfold_tweet \u0026lt;- crossfold_log(tweet_train, 10)\rmod_1 \u0026lt;- caret::train(blueprint_tweet, data = tweet_train, method = \u0026quot;glm\u0026quot;, trControl = crossfold_tweet)\r## Warning in train.recipe(blueprint_tweet, data = tweet_train, method = \u0026quot;glm\u0026quot;, :\r## The metric \u0026quot;Accuracy\u0026quot; was not in the result set. logLoss will be used instead.\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\r## Warning: glm.fit: algorithm did not converge\r## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\rpredicted_test \u0026lt;- predict(mod_1, tweet_test) %\u0026gt;% as.numeric() %\u0026gt;% -1 #subtracting 1 b/c factor levels came out as 1/2 rather than 0/1\r## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\r## prediction from a rank-deficient fit may be misleading\robserved_test \u0026lt;- tweet_test$sentiment %\u0026gt;% as.numeric()\r##Eval Metric/s\rprint(\u0026quot;Error Variance\u0026quot;)\r## [1] \u0026quot;Error Variance\u0026quot;\rRMSE_test \u0026lt;- RMSE(predicted_test, observed_test)\rcat(\u0026quot;Test RMSEA is\u0026quot;, RMSE_test, \u0026quot;\\n\u0026quot;)\r## Test RMSEA is 0.5477226\rmae_test \u0026lt;- MAE(predicted_test, observed_test)\rcat(\u0026quot;Test MAE is\u0026quot;, mae_test, \u0026quot;\\n\u0026quot;)\r## Test MAE is 0.3\rprint(\u0026quot;Accuracy and Prediction Stats\u0026quot;)\r## [1] \u0026quot;Accuracy and Prediction Stats\u0026quot;\r#LogLoss\rLL \u0026lt;- logLoss(observed_test, predicted_test)\rcat(\u0026quot;LogLoss is\u0026quot;, LL, \u0026quot;\\n\u0026quot;)\r## LogLoss is 10.36175\r#AUC\rAUC \u0026lt;- auc(observed_test, predicted_test)\rcat(\u0026quot;Area Under Curve is\u0026quot;, AUC, \u0026quot;\\n\u0026quot;)\r## Area Under Curve is 0.6993452\r#Accuracy\rACC \u0026lt;- accuracy(observed_test, predicted_test)\rcat(\u0026quot;Accuracy is\u0026quot;, ACC, \u0026quot;\\n\u0026quot;)\r## Accuracy is 0.7\r#True Positive Rate\rTPR \u0026lt;- tpr(observed_test, predicted_test, cutoff = .5)\rcat(\u0026quot;True Positive Rate is\u0026quot;, TPR, \u0026quot;\\n\u0026quot;)\r## True Positive Rate is 0.6853147\r#True Negative Rate\rTNR \u0026lt;- tnr(observed_test, predicted_test, cutoff = .5)\rcat(\u0026quot;True Negative Rate is\u0026quot;, TNR, \u0026quot;\\n\u0026quot;)\r## True Negative Rate is 0.7133758\r#Precision\rPRE \u0026lt;- precision(observed_test, predicted_test, cutoff = .5)\rcat(\u0026quot;Precision is\u0026quot;, PRE, \u0026quot;\\n\u0026quot;)\r## Precision is 0.6853147\rmod_1_stats \u0026lt;- c(\u0026quot;Unregularized Regression\u0026quot;, LL, AUC, TPR, TNR, PRE)\rGot errors for this, glm algorithm didn’t emerge. Model performance stats - LogLoss is 10.36175 Area Under Curve is 0.6993452 Accuracy is 0.7 True Positive Rate is 0.6853147 True Negative Rate is 0.7133758 Precision is 0.6853147\nThis first prediction model is a little better than chance (which would be ~.5 for prediction metrics). Accuracy stats for both TPR and TNR are similar, meaning similar chance of being identified correctly as positive, when positive tweet, and negative when negative tweet. Tweets will be classified correctly ~70% of time.\nTask 1.3.\rUse the caret::train() function to train a model with 10-fold cross-validation for predicting the probability of sentiment being positive using logistic regression with ridge penalty. Try different values of ridge penalty to decide the optimal value. Use logLoss as a metric for optimization. Plot the results, and report the optimal value of ridge penalty.\ncrossfold_tweet \u0026lt;- crossfold_log(tweet_train, 10)\rridge_grid \u0026lt;- data.frame(alpha = 0, lambda = c(seq(0.21320, 0.21332, .000001)))\rmod_2 \u0026lt;- caret::train(blueprint_tweet, data = tweet_train,\rmethod = \u0026quot;glmnet\u0026quot;,\rfamily = \u0026#39;binomial\u0026#39;,\rmetric = \u0026#39;logLoss\u0026#39;,\rtrControl = crossfold_tweet,\rtuneGrid = ridge_grid)\rmod_2$bestTune\r## alpha lambda\r## 51 0 0.21325\rpredicted_test \u0026lt;- predict(mod_2, tweet_test) %\u0026gt;% as.numeric() %\u0026gt;% -1 #subtracting 1 b/c factor levels came out as 1/2 rather than 0/1\robserved_test \u0026lt;- tweet_test$sentiment %\u0026gt;% as.numeric()\rplot(mod_2)\r#LogLoss\rLL \u0026lt;- logLoss(observed_test, predicted_test)\r#AUC\rAUC \u0026lt;- auc(observed_test, predicted_test)\r#Accuracy\rACC \u0026lt;- accuracy(observed_test, predicted_test)\r#True Positive Rate\rTPR \u0026lt;- tpr(observed_test, predicted_test, cutoff = .5)\r#True Negative Rate\rTNR \u0026lt;- tnr(observed_test, predicted_test, cutoff = .5)\r#Precision\rPRE \u0026lt;- precision(observed_test, predicted_test, cutoff = .5)\rmod_2_stats \u0026lt;- c(\u0026quot;Ridge Regression\u0026quot;, LL, AUC, TPR, TNR, PRE)\r.001 to 3 by .01. (Used .211)\r.2 to .3 by .0005 (Used 0.2135)\r.212 to .216 by .00003 (Used 0.21326)\r0.21320 to 0.21332 by .000001 (Used 0.21325)\rStopped since this seems to be narrowing in pretty close to optimized value. Optimal lambda value = 0.21325\nTask 1.4.\rUse the caret::train() function to train a model with 10-fold cross-validation for predicting the probability of sentiment being positive using logistic regression with lasso penalty. Try different values of lasso penalty to decide optimal value. Use logLoss as a metric for optimization. Plot the results, and report the optimal value of lasso penalty.\ncrossfold_tweet \u0026lt;- crossfold_log(tweet_train, 10)\rlasso_grid \u0026lt;- data.frame(alpha = 1, lambda = c(seq(.01075, .0108, .0000001)))\rmod_3 \u0026lt;- caret::train(blueprint_tweet, data = tweet_train, method = \u0026quot;glmnet\u0026quot;,\rfamily = \u0026#39;binomial\u0026#39;,\rmetric = \u0026#39;logLoss\u0026#39;,\rtrControl = crossfold_tweet,\rtuneGrid = lasso_grid)\rmod_3$bestTune\r## alpha lambda\r## 391 1 0.010789\rpredicted_test \u0026lt;- predict(mod_3, tweet_test) %\u0026gt;% as.numeric() %\u0026gt;% -1 #subtracting 1 b/c factor levels came out as 1/2 rather than 0/1\robserved_test \u0026lt;- tweet_test$sentiment %\u0026gt;% as.numeric()\rplot(mod_3)\r#LogLoss\rLL \u0026lt;- logLoss(observed_test, predicted_test)\r#AUC\rAUC \u0026lt;- auc(observed_test, predicted_test)\r#Accuracy\rACC \u0026lt;- accuracy(observed_test, predicted_test)\r#True Positive Rate\rTPR \u0026lt;- tpr(observed_test, predicted_test, cutoff = .5)\r#True Negative Rate\rTNR \u0026lt;- tnr(observed_test, predicted_test, cutoff = .5)\r#Precision\rPRE \u0026lt;- precision(observed_test, predicted_test, cutoff = .5)\rmod_3_stats \u0026lt;- c(\u0026quot;Lasso Regression\u0026quot;, LL, AUC, TPR, TNR, PRE)\rTest parameter sets: 1) 0 to 3 by .01 (Used 0.01) 2) .0001 to .5 by .004 (Used 0.0121) 3) 0.01 to .02, by .00005 (Used 0.0108) 4) 0.01 to .012 by .000005 (Used 0.010785) 5) .0107 to .0109 by .0000005 (Used 0.010789) 6) .01075 to .0108 by .0000001 (Used 0.010789)\nStopped since it seems like a local minimum has been reached. Optimal lambda value is 0.010789.\nTask 1.5\rEvaluate the performance of the models in 1.2, 1.3, and 1.4 on the test dataset. Calculate and report logLoss (LL), area under the reciever operating characteristic curver (AUC), overall accuracy (ACC), true positive rate (TPR), true negative rate (TNR), and precision (PRE) for three models. When calculating ACC, TPR, TNR, and PRE, assume that we use a cut-off value of 0.5 for the predicted probabilities. Summarize these numbers in a table like the following. Decide and comment on which model you would use to predict sentiment of a tweet moving forward.\nStats - LL AUC ACC TPR TNR PRE\nRegression Types - Logistic Regression\nLogistic Regression with Ridge Penalty\nLogistic Regression with Lasso Penalty\nnames \u0026lt;- c(\u0026quot;LL\u0026quot;, \u0026quot;AUC\u0026quot;, \u0026quot;ACC\u0026quot;, \u0026quot;TNR\u0026quot;, \u0026quot;PRE\u0026quot;)\rprint(rbind(names, mod_1_stats, mod_2_stats, mod_3_stats))\r## Warning in rbind(names, mod_1_stats, mod_2_stats, mod_3_stats): number of\r## columns of result is not a multiple of vector length (arg 1)\r## [,1] [,2] [,3] ## names \u0026quot;LL\u0026quot; \u0026quot;AUC\u0026quot; \u0026quot;ACC\u0026quot; ## mod_1_stats \u0026quot;Unregularized Regression\u0026quot; \u0026quot;10.3617528580877\u0026quot; \u0026quot;0.699345240746515\u0026quot;\r## mod_2_stats \u0026quot;Ridge Regression\u0026quot; \u0026quot;6.56244747477605\u0026quot; \u0026quot;0.81005300432052\u0026quot; ## mod_3_stats \u0026quot;Lasso Regression\u0026quot; \u0026quot;6.79269798810115\u0026quot; \u0026quot;0.802124626965391\u0026quot;\r## [,4] [,5] [,6] ## names \u0026quot;TNR\u0026quot; \u0026quot;PRE\u0026quot; \u0026quot;LL\u0026quot; ## mod_1_stats \u0026quot;0.685314685314685\u0026quot; \u0026quot;0.713375796178344\u0026quot; \u0026quot;0.685314685314685\u0026quot;\r## mod_2_stats \u0026quot;0.811188811188811\u0026quot; \u0026quot;0.808917197452229\u0026quot; \u0026quot;0.794520547945205\u0026quot;\r## mod_3_stats \u0026quot;0.776223776223776\u0026quot; \u0026quot;0.828025477707006\u0026quot; \u0026quot;0.804347826086957\u0026quot;\rI would use ridge model, due to higher accuracy. There is a slight tradeoff w/ preision and area under curve with using lasso model.\nTask 1.6\rFor the model you decided in 1.5, find and report the most important 10 predictors of sentiment and th0eir coefficients. Briefly comment which variables seem to be the most important predictors.\n#install.packages(\u0026#39;vip\u0026#39;)\rrequire(vip)\r## Loading required package: vip\r## ## Attaching package: \u0026#39;vip\u0026#39;\r## The following object is masked from \u0026#39;package:utils\u0026#39;:\r## ## vi\rvip(mod_2, num_features = 10, geom = \u0026quot;point\u0026quot;) + theme_bw()\rIn this model, it seems the month, date, and hour variables are all strong predictors, as well as a few word embeddings.\nTask 1.7.\rBelow are the two tweets I picked from my timeline. Use the model you decided in Task 1.5 to predict a probability that the sentiment being positive for these tweets. You are welcome to extract the word embeddings for these tweets by yourself (model: roberta-base, layer=12). Assume that all these tweets are posted on Saturday, May 1, 2021 at 12pm. For convenience, you can also download the dataset from the link below in case you have trouble in extracting the word embeddings.\ntweet1 \u0026lt;- \u0026quot;You are not getting displaced you decide to sell your $800k townhome because a 12-story apartment goes up next door, and then can\u0026#39;t find another $800k townhome in Arlington.\u0026quot;\rtweet2 \u0026lt;- \u0026quot;One cold morning and I\u0026#39;m in holiday mode. Bought gifts for students.\u0026quot;\rnew_tweets \u0026lt;- read.csv(\u0026#39;https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/toy_tweet_embeddings.csv\u0026#39;,header=TRUE)\rpred_tweet \u0026lt;- predict(mod_2, new_tweets, type = \u0026quot;response\u0026quot;)\rcbind(new_tweets$tweet, pred_tweet)\r## new_tweets$tweet\r## 1 You are not getting displaced you decide to sell your $800k townhome because a 12-story apartment goes up next door, and then can\u0026#39;t find another $800k townhome in Arlington.\r## 2 One cold morning and I\u0026#39;m in holiday mode. Bought gifts for students.\r## Negative Positive\r## 1 0.5295685 0.4704315\r## 2 0.7190989 0.2809011\rPredicted probability positive Tweet 1 = .47 Predicted probability positive Tweet 1 = .28\nTask 1.8.\rLet’s do an experiment and test whether or not the model is biased against certain groups when detecting sentiment of a given text. Below you will find 10 hypothetical tweets with an identical structure. The only thing that changes from tweet to tweet is the subject.\nbias_check \u0026lt;- read.csv(\u0026#39;https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/bias_check_tweet_embeddings.csv\u0026#39;,header=TRUE)\rpred_tweet2 \u0026lt;- predict(mod_2, bias_check, type = \u0026quot;response\u0026quot;)\rcbind(bias_check$tweet, pred_tweet2) %\u0026gt;% arrange(Negative)\r## bias_check$tweet Negative Positive\r## 1 Muslims are so annoying! 0.2906895 0.7093105\r## 2 American people are so annoying! 0.3157537 0.6842463\r## 3 Russian people are so annoying! 0.3478506 0.6521494\r## 4 Turkish people are so annoying! 0.3631084 0.6368916\r## 5 Jews are so annoying! 0.3863735 0.6136265\r## 6 Japanese people are so annoying! 0.4188987 0.5811013\r## 7 French people are so annoying! 0.4197578 0.5802422\r## 8 Christians are so annoying! 0.4233919 0.5766081\r## 9 Buddhists are so annoying! 0.4383777 0.5616223\r## 10 Atheists are so annoying! 0.4417626 0.5582374\rIt seems like algorithm is most biased towards Muslim people, and least biased towards Atheist people. This statement is least likely to have a negative sentiment when Muslim people are the group, and most likely to have a negative sentiment when Atheist is subject.\nPart 2: Linear Regression\rLoad and prep data\r# Load the following packages needed for modeling in this assignment\rrequire(caret)\rrequire(recipes)\rrequire(finalfit)\rrequire(glmnet)\r# Import the oregon dataset\r#oregon \u0026lt;- read.csv(\u0026#39;https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/hw1_oregon_final.csv\u0026#39;,header=TRUE)\roregon \u0026lt;- read.csv(\u0026quot;Oregon_Data.csv\u0026quot;)\r# Recipe for the oregon dataset\routcome \u0026lt;- \u0026#39;score\u0026#39;\rid \u0026lt;- \u0026#39;id\u0026#39;\rcategorical \u0026lt;- c(\u0026#39;sex\u0026#39;,\u0026#39;ethnic_cd\u0026#39;,\u0026#39;tst_bnch\u0026#39;,\u0026#39;migrant_ed_fg\u0026#39;,\u0026#39;ind_ed_fg\u0026#39;,\r\u0026#39;sp_ed_fg\u0026#39;,\u0026#39;tag_ed_fg\u0026#39;,\u0026#39;econ_dsvntg\u0026#39;,\u0026#39;stay_in_dist\u0026#39;,\r\u0026#39;stay_in_schl\u0026#39;,\u0026#39;dist_sped\u0026#39;,\u0026#39;trgt_assist_fg\u0026#39;,\r\u0026#39;ayp_dist_partic\u0026#39;,\u0026#39;ayp_schl_partic\u0026#39;,\u0026#39;ayp_dist_prfrm\u0026#39;,\r\u0026#39;ayp_schl_prfrm\u0026#39;,\u0026#39;rc_dist_partic\u0026#39;,\u0026#39;rc_schl_partic\u0026#39;,\r\u0026#39;rc_dist_prfrm\u0026#39;,\u0026#39;rc_schl_prfrm\u0026#39;,\u0026#39;grp_rpt_dist_partic\u0026#39;,\r\u0026#39;grp_rpt_schl_partic\u0026#39;,\u0026#39;grp_rpt_dist_prfrm\u0026#39;,\r\u0026#39;grp_rpt_schl_prfrm\u0026#39;)\rnumeric \u0026lt;- c(\u0026#39;enrl_grd\u0026#39;)\rcyclic \u0026lt;- c(\u0026#39;date\u0026#39;,\u0026#39;month\u0026#39;)\rblueprint_oregon \u0026lt;- recipe(x = oregon,\rvars = c(outcome,categorical,numeric,cyclic),\rroles = c(\u0026#39;outcome\u0026#39;,rep(\u0026#39;predictor\u0026#39;,27))) %\u0026gt;%\rstep_indicate_na(all_of(categorical),all_of(numeric)) %\u0026gt;%\rstep_zv(all_numeric()) %\u0026gt;%\rstep_impute_mean(all_of(numeric)) %\u0026gt;%\rstep_impute_mode(all_of(categorical)) %\u0026gt;%\rstep_harmonic(\u0026#39;date\u0026#39;,frequency=1,cycle_size=31,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_harmonic(\u0026#39;month\u0026#39;,frequency=1,cycle_size=12,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_ns(\u0026#39;enrl_grd\u0026#39;,deg_free=3) %\u0026gt;%\rstep_normalize(c(paste0(numeric,\u0026#39;_ns_1\u0026#39;),paste0(numeric,\u0026#39;_ns_2\u0026#39;),paste0(numeric,\u0026#39;_ns_3\u0026#39;))) %\u0026gt;%\rstep_normalize(c(\u0026quot;date_sin_1\u0026quot;,\u0026quot;date_cos_1\u0026quot;,\u0026quot;month_sin_1\u0026quot;,\u0026quot;month_cos_1\u0026quot;)) %\u0026gt;%\rstep_dummy(all_of(categorical),one_hot=TRUE) %\u0026gt;%\rstep_rm(c(\u0026#39;date\u0026#39;,\u0026#39;month\u0026#39;))\rprint(blueprint_oregon %\u0026gt;% prep() %\u0026gt;% summary)\r## # A tibble: 73 x 4\r## variable type role source ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 score numeric outcome original\r## 2 na_ind_ind_ed_fg numeric predictor derived ## 3 na_ind_sp_ed_fg numeric predictor derived ## 4 na_ind_tag_ed_fg numeric predictor derived ## 5 na_ind_econ_dsvntg numeric predictor derived ## 6 na_ind_stay_in_dist numeric predictor derived ## 7 na_ind_stay_in_schl numeric predictor derived ## 8 na_ind_dist_sped numeric predictor derived ## 9 na_ind_trgt_assist_fg numeric predictor derived ## 10 date_sin_1 numeric predictor derived ## # ... with 63 more rows\rblueprint_oregon_ridge \u0026lt;- recipe(x = oregon,\rvars = c(outcome,categorical,numeric,cyclic),\rroles = c(\u0026#39;outcome\u0026#39;,rep(\u0026#39;predictor\u0026#39;,27))) %\u0026gt;%\rstep_indicate_na(all_of(categorical),all_of(numeric)) %\u0026gt;%\rstep_zv(all_numeric()) %\u0026gt;%\rstep_impute_mean(all_of(numeric)) %\u0026gt;%\rstep_impute_mode(all_of(categorical)) %\u0026gt;%\rstep_harmonic(\u0026#39;date\u0026#39;,frequency=1,cycle_size=31,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_harmonic(\u0026#39;month\u0026#39;,frequency=1,cycle_size=12,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_ns(\u0026#39;enrl_grd\u0026#39;,deg_free=3) %\u0026gt;%\rstep_normalize(c(paste0(numeric,\u0026#39;_ns_1\u0026#39;),paste0(numeric,\u0026#39;_ns_2\u0026#39;),paste0(numeric,\u0026#39;_ns_3\u0026#39;))) %\u0026gt;%\rstep_normalize(c(\u0026quot;date_sin_1\u0026quot;,\u0026quot;date_cos_1\u0026quot;,\u0026quot;month_sin_1\u0026quot;,\u0026quot;month_cos_1\u0026quot;)) %\u0026gt;%\rstep_dummy(all_of(categorical),one_hot=TRUE) %\u0026gt;%\rstep_rm(c(\u0026#39;date\u0026#39;,\u0026#39;month\u0026#39;)) %\u0026gt;%\rstep_normalize(everything())\rTask 2.1.\rCheck the dataset for missingness. If there is any variable with more than 75% missingness, remove these variables.\nff_glimpse(oregon)\r## $Continuous\r## label var_type n missing_n missing_percent mean sd\r## X X \u0026lt;int\u0026gt; 189426 0 0.0 94713.5 54682.7\r## id id \u0026lt;int\u0026gt; 189426 0 0.0 126190.5 72879.8\r## enrl_grd enrl_grd \u0026lt;int\u0026gt; 189426 0 0.0 5.5 1.7\r## score score \u0026lt;int\u0026gt; 189426 0 0.0 2499.0 115.8\r## month month \u0026lt;int\u0026gt; 189426 0 0.0 5.0 0.4\r## date date \u0026lt;int\u0026gt; 189426 0 0.0 17.4 8.4\r## min quartile_25 median quartile_75 max\r## X 1.0 47357.2 94713.5 142069.8 189426.0\r## id 1.0 63135.2 126095.5 189261.8 252568.0\r## enrl_grd 3.0 4.0 5.0 7.0 8.0\r## score 1601.0 2421.0 2498.0 2576.0 3550.0\r## month 2.0 5.0 5.0 5.0 6.0\r## date 1.0 10.0 18.0 24.0 31.0\r## ## $Categorical\r## label var_type n missing_n\r## sex sex \u0026lt;chr\u0026gt; 189426 0\r## ethnic_cd ethnic_cd \u0026lt;chr\u0026gt; 189426 0\r## tst_bnch tst_bnch \u0026lt;chr\u0026gt; 189426 0\r## migrant_ed_fg migrant_ed_fg \u0026lt;chr\u0026gt; 189426 0\r## ind_ed_fg ind_ed_fg \u0026lt;chr\u0026gt; 189365 61\r## sp_ed_fg sp_ed_fg \u0026lt;chr\u0026gt; 189365 61\r## tag_ed_fg tag_ed_fg \u0026lt;chr\u0026gt; 188963 463\r## econ_dsvntg econ_dsvntg \u0026lt;chr\u0026gt; 188895 531\r## stay_in_dist stay_in_dist \u0026lt;chr\u0026gt; 188963 463\r## stay_in_schl stay_in_schl \u0026lt;chr\u0026gt; 188963 463\r## dist_sped dist_sped \u0026lt;chr\u0026gt; 188963 463\r## trgt_assist_fg trgt_assist_fg \u0026lt;chr\u0026gt; 188956 470\r## ayp_dist_partic ayp_dist_partic \u0026lt;chr\u0026gt; 189426 0\r## ayp_schl_partic ayp_schl_partic \u0026lt;chr\u0026gt; 189426 0\r## ayp_dist_prfrm ayp_dist_prfrm \u0026lt;chr\u0026gt; 189426 0\r## ayp_schl_prfrm ayp_schl_prfrm \u0026lt;chr\u0026gt; 189426 0\r## rc_dist_partic rc_dist_partic \u0026lt;chr\u0026gt; 189426 0\r## rc_schl_partic rc_schl_partic \u0026lt;chr\u0026gt; 189426 0\r## rc_dist_prfrm rc_dist_prfrm \u0026lt;chr\u0026gt; 189426 0\r## rc_schl_prfrm rc_schl_prfrm \u0026lt;chr\u0026gt; 189426 0\r## grp_rpt_dist_partic grp_rpt_dist_partic \u0026lt;chr\u0026gt; 189426 0\r## grp_rpt_schl_partic grp_rpt_schl_partic \u0026lt;chr\u0026gt; 189426 0\r## grp_rpt_dist_prfrm grp_rpt_dist_prfrm \u0026lt;chr\u0026gt; 189426 0\r## grp_rpt_schl_prfrm grp_rpt_schl_prfrm \u0026lt;chr\u0026gt; 189426 0\r## missing_percent levels_n levels levels_count levels_percent\r## sex 0.0 2 - - -\r## ethnic_cd 0.0 7 - - -\r## tst_bnch 0.0 6 - - -\r## migrant_ed_fg 0.0 2 - - -\r## ind_ed_fg 0.0 2 - - -\r## sp_ed_fg 0.0 2 - - -\r## tag_ed_fg 0.2 2 - - -\r## econ_dsvntg 0.3 2 - - -\r## stay_in_dist 0.2 2 - - -\r## stay_in_schl 0.2 2 - - -\r## dist_sped 0.2 2 - - -\r## trgt_assist_fg 0.2 2 - - -\r## ayp_dist_partic 0.0 2 - - -\r## ayp_schl_partic 0.0 2 - - -\r## ayp_dist_prfrm 0.0 2 - - -\r## ayp_schl_prfrm 0.0 2 - - -\r## rc_dist_partic 0.0 2 - - -\r## rc_schl_partic 0.0 2 - - -\r## rc_dist_prfrm 0.0 2 - - -\r## rc_schl_prfrm 0.0 2 - - -\r## grp_rpt_dist_partic 0.0 2 - - -\r## grp_rpt_schl_partic 0.0 2 - - -\r## grp_rpt_dist_prfrm 0.0 2 - - -\r## grp_rpt_schl_prfrm 0.0 2 - - -\rTask 2.2.\rSplit the original data into two subsets: training and test. Let the training data have the 80% of cases and the test data have the 20% of the cases.\nset.seed(11172021) # for reproducibility\rloc \u0026lt;- sample(1:nrow(oregon), round(nrow(oregon) * 0.8))\roregon_train \u0026lt;- oregon[loc, ]\roregon_test \u0026lt;- oregon[-loc, ]\rTask 2.3.\rUse the caret::train() function to train a model with 10-fold cross-validation to predict the scores using linear regression without any regularization. Evaluate the performance of the model on both training and test datasets. Evaluate and report RMSE, R-square, and MAE for both training and test datasets. Is there any evidence of overfitting?\n#making a crossfold training function\rcrossfold \u0026lt;- function(training_data, folds){\r#randomly shuffle data\r# Randomly shuffle the data\rtraining_data = training_data[sample(nrow(training_data)),]\r# Create 10 folds with equal size\rN_folds = cut(seq(1,nrow(training_data)),breaks= folds,labels=FALSE)\r# Create the list for each fold my.indices \u0026lt;- vector(\u0026#39;list\u0026#39;,folds)\rfor(i in 1:folds){\rmy.indices[[i]] \u0026lt;- which(N_folds!=i)\r}\rcv \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;,\rindex = my.indices)\rcv\r}\rcross_or \u0026lt;- crossfold(oregon_train, 10)\ror_mod_1 \u0026lt;- caret::train(blueprint_oregon, data = oregon_train, method = \u0026quot;lm\u0026quot;, trControl = cross_or)\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\ror_mod_1\r## Linear Regression ## ## 151541 samples\r## 29 predictor\r## ## Recipe steps: indicate_na, zv, impute_mean, impute_mode, harmonic,\r## harmonic, ns, normalize, normalize, dummy, rm ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 136386, 136387, 136387, 136387, 136387, 136387, ... ## Resampling results:\r## ## RMSE Rsquared MAE ## 89.81761 0.4009267 69.57297\r## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE\rpredicted_test \u0026lt;- predict(or_mod_1, oregon_test)\r## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\r## may be misleading\rRMSE_test \u0026lt;- RMSE(predicted_test, oregon_test$score)\rcat(\u0026quot;Test RMSEA is\u0026quot;, RMSE_test, \u0026quot;\\n\u0026quot;)\r## Test RMSEA is 88.96643\rrsq_test \u0026lt;- cor(predicted_test, oregon_test$score)^2\rcat(\u0026quot;Test R-squared is\u0026quot;, rsq_test, \u0026quot;\\n\u0026quot;)\r## Test R-squared is 0.4013898\rmae_test \u0026lt;- MAE(predicted_test, oregon_test$score)\rcat(\u0026quot;Test MAE is\u0026quot;, mae_test, \u0026quot;\\n\u0026quot;)\r## Test MAE is 69.18454\rmod_1_stats \u0026lt;- c(\u0026quot;Unregularized Regression\u0026quot;, RMSE_test, rsq_test, mae_test)\rI recieved an error: rank deficient model. Performance statistics between train and test datasets appear quite similar. Interestingly, less error variance for test data. So it seems, no, model is not overfit to training dataset.\nTask 2.4.\rUse the caret::train() function to train a model with 10-fold cross-validation to predict the scores using ridge regression. Try different values of lambda to decide optimal value. Evaluate the performance of the model on the test dataset, and report RMSE, R-square, and MAE. Does ridge regression provide any improvement over linear regression with no regularization?\nridge_grid \u0026lt;- data.frame(alpha = 0, lambda = seq(.003, .04, .0001))\rcross_or \u0026lt;- crossfold(oregon_train, 10)\ror_mod_2 \u0026lt;- caret::train(blueprint_oregon_ridge, data = oregon_train, method = \u0026quot;glmnet\u0026quot;, trControl = cross_or, tuneGrid = ridge_grid)\ror_mod_2\r## glmnet ## ## 151541 samples\r## 29 predictor\r## ## Recipe steps: indicate_na, zv, impute_mean, impute_mode, harmonic,\r## harmonic, ns, normalize, normalize, dummy, rm, normalize ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 136386, 136387, 136387, 136387, 136387, 136387, ... ## Resampling results across tuning parameters:\r## ## lambda RMSE Rsquared MAE ## 0.0030 0.7731359 0.4023295 0.5996013\r## 0.0031 0.7731359 0.4023295 0.5996013\r## 0.0032 0.7731359 0.4023295 0.5996013\r## 0.0033 0.7731359 0.4023295 0.5996013\r## 0.0034 0.7731359 0.4023295 0.5996013\r## 0.0035 0.7731359 0.4023295 0.5996013\r## 0.0036 0.7731359 0.4023295 0.5996013\r## 0.0037 0.7731359 0.4023295 0.5996013\r## 0.0038 0.7731359 0.4023295 0.5996013\r## 0.0039 0.7731359 0.4023295 0.5996013\r## 0.0040 0.7731359 0.4023295 0.5996013\r## 0.0041 0.7731359 0.4023295 0.5996013\r## 0.0042 0.7731359 0.4023295 0.5996013\r## 0.0043 0.7731359 0.4023295 0.5996013\r## 0.0044 0.7731359 0.4023295 0.5996013\r## 0.0045 0.7731359 0.4023295 0.5996013\r## 0.0046 0.7731359 0.4023295 0.5996013\r## 0.0047 0.7731359 0.4023295 0.5996013\r## 0.0048 0.7731359 0.4023295 0.5996013\r## 0.0049 0.7731359 0.4023295 0.5996013\r## 0.0050 0.7731359 0.4023295 0.5996013\r## 0.0051 0.7731359 0.4023295 0.5996013\r## 0.0052 0.7731359 0.4023295 0.5996013\r## 0.0053 0.7731359 0.4023295 0.5996013\r## 0.0054 0.7731359 0.4023295 0.5996013\r## 0.0055 0.7731359 0.4023295 0.5996013\r## 0.0056 0.7731359 0.4023295 0.5996013\r## 0.0057 0.7731359 0.4023295 0.5996013\r## 0.0058 0.7731359 0.4023295 0.5996013\r## 0.0059 0.7731359 0.4023295 0.5996013\r## 0.0060 0.7731359 0.4023295 0.5996013\r## 0.0061 0.7731359 0.4023295 0.5996013\r## 0.0062 0.7731359 0.4023295 0.5996013\r## 0.0063 0.7731359 0.4023295 0.5996013\r## 0.0064 0.7731359 0.4023295 0.5996013\r## 0.0065 0.7731359 0.4023295 0.5996013\r## 0.0066 0.7731359 0.4023295 0.5996013\r## 0.0067 0.7731359 0.4023295 0.5996013\r## 0.0068 0.7731359 0.4023295 0.5996013\r## 0.0069 0.7731359 0.4023295 0.5996013\r## 0.0070 0.7731359 0.4023295 0.5996013\r## 0.0071 0.7731359 0.4023295 0.5996013\r## 0.0072 0.7731359 0.4023295 0.5996013\r## 0.0073 0.7731359 0.4023295 0.5996013\r## 0.0074 0.7731359 0.4023295 0.5996013\r## 0.0075 0.7731359 0.4023295 0.5996013\r## 0.0076 0.7731359 0.4023295 0.5996013\r## 0.0077 0.7731359 0.4023295 0.5996013\r## 0.0078 0.7731359 0.4023295 0.5996013\r## 0.0079 0.7731359 0.4023295 0.5996013\r## 0.0080 0.7731359 0.4023295 0.5996013\r## 0.0081 0.7731359 0.4023295 0.5996013\r## 0.0082 0.7731359 0.4023295 0.5996013\r## 0.0083 0.7731359 0.4023295 0.5996013\r## 0.0084 0.7731359 0.4023295 0.5996013\r## 0.0085 0.7731359 0.4023295 0.5996013\r## 0.0086 0.7731359 0.4023295 0.5996013\r## 0.0087 0.7731359 0.4023295 0.5996013\r## 0.0088 0.7731359 0.4023295 0.5996013\r## 0.0089 0.7731359 0.4023295 0.5996013\r## 0.0090 0.7731359 0.4023295 0.5996013\r## 0.0091 0.7731359 0.4023295 0.5996013\r## 0.0092 0.7731359 0.4023295 0.5996013\r## 0.0093 0.7731359 0.4023295 0.5996013\r## 0.0094 0.7731359 0.4023295 0.5996013\r## 0.0095 0.7731359 0.4023295 0.5996013\r## 0.0096 0.7731359 0.4023295 0.5996013\r## 0.0097 0.7731359 0.4023295 0.5996013\r## 0.0098 0.7731359 0.4023295 0.5996013\r## 0.0099 0.7731359 0.4023295 0.5996013\r## 0.0100 0.7731359 0.4023295 0.5996013\r## 0.0101 0.7731359 0.4023295 0.5996013\r## 0.0102 0.7731359 0.4023295 0.5996013\r## 0.0103 0.7731359 0.4023295 0.5996013\r## 0.0104 0.7731359 0.4023295 0.5996013\r## 0.0105 0.7731359 0.4023295 0.5996013\r## 0.0106 0.7731359 0.4023295 0.5996013\r## 0.0107 0.7731359 0.4023295 0.5996013\r## 0.0108 0.7731359 0.4023295 0.5996013\r## 0.0109 0.7731359 0.4023295 0.5996013\r## 0.0110 0.7731359 0.4023295 0.5996013\r## 0.0111 0.7731359 0.4023295 0.5996013\r## 0.0112 0.7731359 0.4023295 0.5996013\r## 0.0113 0.7731359 0.4023295 0.5996013\r## 0.0114 0.7731359 0.4023295 0.5996013\r## 0.0115 0.7731359 0.4023295 0.5996013\r## 0.0116 0.7731359 0.4023295 0.5996013\r## 0.0117 0.7731359 0.4023295 0.5996013\r## 0.0118 0.7731359 0.4023295 0.5996013\r## 0.0119 0.7731359 0.4023295 0.5996013\r## 0.0120 0.7731359 0.4023295 0.5996013\r## 0.0121 0.7731359 0.4023295 0.5996013\r## 0.0122 0.7731359 0.4023295 0.5996013\r## 0.0123 0.7731359 0.4023295 0.5996013\r## 0.0124 0.7731359 0.4023295 0.5996013\r## 0.0125 0.7731359 0.4023295 0.5996013\r## 0.0126 0.7731359 0.4023295 0.5996013\r## 0.0127 0.7731359 0.4023295 0.5996013\r## 0.0128 0.7731359 0.4023295 0.5996013\r## 0.0129 0.7731359 0.4023295 0.5996013\r## 0.0130 0.7731359 0.4023295 0.5996013\r## 0.0131 0.7731359 0.4023295 0.5996013\r## 0.0132 0.7731359 0.4023295 0.5996013\r## 0.0133 0.7731359 0.4023295 0.5996013\r## 0.0134 0.7731359 0.4023295 0.5996013\r## 0.0135 0.7731359 0.4023295 0.5996013\r## 0.0136 0.7731359 0.4023295 0.5996013\r## 0.0137 0.7731359 0.4023295 0.5996013\r## 0.0138 0.7731359 0.4023295 0.5996013\r## 0.0139 0.7731359 0.4023295 0.5996013\r## 0.0140 0.7731359 0.4023295 0.5996013\r## 0.0141 0.7731359 0.4023295 0.5996013\r## 0.0142 0.7731359 0.4023295 0.5996013\r## 0.0143 0.7731359 0.4023295 0.5996013\r## 0.0144 0.7731359 0.4023295 0.5996013\r## 0.0145 0.7731359 0.4023295 0.5996013\r## 0.0146 0.7731359 0.4023295 0.5996013\r## 0.0147 0.7731359 0.4023295 0.5996013\r## 0.0148 0.7731359 0.4023295 0.5996013\r## 0.0149 0.7731359 0.4023295 0.5996013\r## 0.0150 0.7731359 0.4023295 0.5996013\r## 0.0151 0.7731359 0.4023295 0.5996013\r## 0.0152 0.7731359 0.4023295 0.5996013\r## 0.0153 0.7731359 0.4023295 0.5996013\r## 0.0154 0.7731359 0.4023295 0.5996013\r## 0.0155 0.7731359 0.4023295 0.5996013\r## 0.0156 0.7731359 0.4023295 0.5996013\r## 0.0157 0.7731359 0.4023295 0.5996013\r## 0.0158 0.7731359 0.4023295 0.5996013\r## 0.0159 0.7731359 0.4023295 0.5996013\r## 0.0160 0.7731359 0.4023295 0.5996013\r## 0.0161 0.7731359 0.4023295 0.5996013\r## 0.0162 0.7731359 0.4023295 0.5996013\r## 0.0163 0.7731359 0.4023295 0.5996013\r## 0.0164 0.7731359 0.4023295 0.5996013\r## 0.0165 0.7731359 0.4023295 0.5996013\r## 0.0166 0.7731359 0.4023295 0.5996013\r## 0.0167 0.7731359 0.4023295 0.5996013\r## 0.0168 0.7731359 0.4023295 0.5996013\r## 0.0169 0.7731359 0.4023295 0.5996013\r## 0.0170 0.7731359 0.4023295 0.5996013\r## 0.0171 0.7731359 0.4023295 0.5996013\r## 0.0172 0.7731359 0.4023295 0.5996013\r## 0.0173 0.7731359 0.4023295 0.5996013\r## 0.0174 0.7731359 0.4023295 0.5996013\r## 0.0175 0.7731359 0.4023295 0.5996013\r## 0.0176 0.7731359 0.4023295 0.5996013\r## 0.0177 0.7731359 0.4023295 0.5996013\r## 0.0178 0.7731359 0.4023295 0.5996013\r## 0.0179 0.7731359 0.4023295 0.5996013\r## 0.0180 0.7731359 0.4023295 0.5996013\r## 0.0181 0.7731359 0.4023295 0.5996013\r## 0.0182 0.7731359 0.4023295 0.5996013\r## 0.0183 0.7731359 0.4023295 0.5996013\r## 0.0184 0.7731359 0.4023295 0.5996013\r## 0.0185 0.7731359 0.4023295 0.5996013\r## 0.0186 0.7731359 0.4023295 0.5996013\r## 0.0187 0.7731359 0.4023295 0.5996013\r## 0.0188 0.7731359 0.4023295 0.5996013\r## 0.0189 0.7731359 0.4023295 0.5996013\r## 0.0190 0.7731359 0.4023295 0.5996013\r## 0.0191 0.7731359 0.4023295 0.5996013\r## 0.0192 0.7731359 0.4023295 0.5996013\r## 0.0193 0.7731359 0.4023295 0.5996013\r## 0.0194 0.7731359 0.4023295 0.5996013\r## 0.0195 0.7731359 0.4023295 0.5996013\r## 0.0196 0.7731359 0.4023295 0.5996013\r## 0.0197 0.7731359 0.4023295 0.5996013\r## 0.0198 0.7731359 0.4023295 0.5996013\r## 0.0199 0.7731359 0.4023295 0.5996013\r## 0.0200 0.7731359 0.4023295 0.5996013\r## 0.0201 0.7731359 0.4023295 0.5996013\r## 0.0202 0.7731359 0.4023295 0.5996013\r## 0.0203 0.7731359 0.4023295 0.5996013\r## 0.0204 0.7731359 0.4023295 0.5996013\r## 0.0205 0.7731359 0.4023295 0.5996013\r## 0.0206 0.7731359 0.4023295 0.5996013\r## 0.0207 0.7731359 0.4023295 0.5996013\r## 0.0208 0.7731359 0.4023295 0.5996013\r## 0.0209 0.7731359 0.4023295 0.5996013\r## 0.0210 0.7731359 0.4023295 0.5996013\r## 0.0211 0.7731359 0.4023295 0.5996013\r## 0.0212 0.7731359 0.4023295 0.5996013\r## 0.0213 0.7731359 0.4023295 0.5996013\r## 0.0214 0.7731359 0.4023295 0.5996013\r## 0.0215 0.7731359 0.4023295 0.5996013\r## 0.0216 0.7731359 0.4023295 0.5996013\r## 0.0217 0.7731359 0.4023295 0.5996013\r## 0.0218 0.7731359 0.4023295 0.5996013\r## 0.0219 0.7731359 0.4023295 0.5996013\r## 0.0220 0.7731359 0.4023295 0.5996013\r## 0.0221 0.7731359 0.4023295 0.5996013\r## 0.0222 0.7731359 0.4023295 0.5996013\r## 0.0223 0.7731359 0.4023295 0.5996013\r## 0.0224 0.7731359 0.4023295 0.5996013\r## 0.0225 0.7731359 0.4023295 0.5996013\r## 0.0226 0.7731359 0.4023295 0.5996013\r## 0.0227 0.7731359 0.4023295 0.5996013\r## 0.0228 0.7731359 0.4023295 0.5996013\r## 0.0229 0.7731359 0.4023295 0.5996013\r## 0.0230 0.7731359 0.4023295 0.5996013\r## 0.0231 0.7731359 0.4023295 0.5996013\r## 0.0232 0.7731359 0.4023295 0.5996013\r## 0.0233 0.7731359 0.4023295 0.5996013\r## 0.0234 0.7731359 0.4023295 0.5996013\r## 0.0235 0.7731359 0.4023295 0.5996013\r## 0.0236 0.7731359 0.4023295 0.5996013\r## 0.0237 0.7731359 0.4023295 0.5996013\r## 0.0238 0.7731359 0.4023295 0.5996013\r## 0.0239 0.7731359 0.4023295 0.5996013\r## 0.0240 0.7731359 0.4023295 0.5996013\r## 0.0241 0.7731359 0.4023295 0.5996013\r## 0.0242 0.7731359 0.4023295 0.5996013\r## 0.0243 0.7731359 0.4023295 0.5996013\r## 0.0244 0.7731359 0.4023295 0.5996013\r## 0.0245 0.7731359 0.4023295 0.5996013\r## 0.0246 0.7731359 0.4023295 0.5996013\r## 0.0247 0.7731359 0.4023295 0.5996013\r## 0.0248 0.7731359 0.4023295 0.5996013\r## 0.0249 0.7731359 0.4023295 0.5996013\r## 0.0250 0.7731359 0.4023295 0.5996013\r## 0.0251 0.7731359 0.4023295 0.5996013\r## 0.0252 0.7731359 0.4023295 0.5996013\r## 0.0253 0.7731359 0.4023295 0.5996013\r## 0.0254 0.7731359 0.4023295 0.5996013\r## 0.0255 0.7731359 0.4023295 0.5996013\r## 0.0256 0.7731359 0.4023295 0.5996013\r## 0.0257 0.7731359 0.4023295 0.5996013\r## 0.0258 0.7731359 0.4023295 0.5996013\r## 0.0259 0.7731359 0.4023295 0.5996013\r## 0.0260 0.7731359 0.4023295 0.5996013\r## 0.0261 0.7731359 0.4023295 0.5996013\r## 0.0262 0.7731359 0.4023295 0.5996013\r## 0.0263 0.7731359 0.4023295 0.5996013\r## 0.0264 0.7731359 0.4023295 0.5996013\r## 0.0265 0.7731359 0.4023295 0.5996013\r## 0.0266 0.7731359 0.4023295 0.5996013\r## 0.0267 0.7731359 0.4023295 0.5996013\r## 0.0268 0.7731359 0.4023295 0.5996013\r## 0.0269 0.7731359 0.4023295 0.5996013\r## 0.0270 0.7731359 0.4023295 0.5996013\r## 0.0271 0.7731359 0.4023295 0.5996013\r## 0.0272 0.7731359 0.4023295 0.5996013\r## 0.0273 0.7731359 0.4023295 0.5996013\r## 0.0274 0.7731359 0.4023295 0.5996013\r## 0.0275 0.7731359 0.4023295 0.5996013\r## 0.0276 0.7731359 0.4023295 0.5996013\r## 0.0277 0.7731359 0.4023295 0.5996013\r## 0.0278 0.7731359 0.4023295 0.5996013\r## 0.0279 0.7731359 0.4023295 0.5996013\r## 0.0280 0.7731359 0.4023295 0.5996013\r## 0.0281 0.7731359 0.4023295 0.5996013\r## 0.0282 0.7731359 0.4023295 0.5996013\r## 0.0283 0.7731359 0.4023295 0.5996013\r## 0.0284 0.7731359 0.4023295 0.5996013\r## 0.0285 0.7731359 0.4023295 0.5996013\r## 0.0286 0.7731359 0.4023295 0.5996013\r## 0.0287 0.7731359 0.4023295 0.5996013\r## 0.0288 0.7731359 0.4023295 0.5996013\r## 0.0289 0.7731359 0.4023295 0.5996013\r## 0.0290 0.7731359 0.4023295 0.5996013\r## 0.0291 0.7731359 0.4023295 0.5996013\r## 0.0292 0.7731359 0.4023295 0.5996013\r## 0.0293 0.7731359 0.4023295 0.5996013\r## 0.0294 0.7731359 0.4023295 0.5996013\r## 0.0295 0.7731359 0.4023295 0.5996013\r## 0.0296 0.7731359 0.4023295 0.5996013\r## 0.0297 0.7731359 0.4023295 0.5996013\r## 0.0298 0.7731359 0.4023295 0.5996013\r## 0.0299 0.7731359 0.4023295 0.5996013\r## 0.0300 0.7731359 0.4023295 0.5996013\r## 0.0301 0.7731359 0.4023295 0.5996013\r## 0.0302 0.7731359 0.4023295 0.5996013\r## 0.0303 0.7731359 0.4023295 0.5996013\r## 0.0304 0.7731359 0.4023295 0.5996013\r## 0.0305 0.7731359 0.4023295 0.5996013\r## 0.0306 0.7731359 0.4023295 0.5996013\r## 0.0307 0.7731359 0.4023295 0.5996013\r## 0.0308 0.7731359 0.4023295 0.5996013\r## 0.0309 0.7731359 0.4023295 0.5996013\r## 0.0310 0.7731359 0.4023295 0.5996013\r## 0.0311 0.7731359 0.4023295 0.5996013\r## 0.0312 0.7731359 0.4023295 0.5996013\r## 0.0313 0.7731359 0.4023295 0.5996013\r## 0.0314 0.7731359 0.4023295 0.5996013\r## 0.0315 0.7731359 0.4023295 0.5996013\r## 0.0316 0.7731359 0.4023295 0.5996013\r## 0.0317 0.7731359 0.4023295 0.5996013\r## 0.0318 0.7731359 0.4023295 0.5996013\r## 0.0319 0.7731359 0.4023295 0.5996013\r## 0.0320 0.7731359 0.4023295 0.5996013\r## 0.0321 0.7731359 0.4023295 0.5996013\r## 0.0322 0.7731359 0.4023295 0.5996013\r## 0.0323 0.7731359 0.4023295 0.5996013\r## 0.0324 0.7731359 0.4023295 0.5996013\r## 0.0325 0.7731359 0.4023295 0.5996013\r## 0.0326 0.7731359 0.4023295 0.5996013\r## 0.0327 0.7731359 0.4023295 0.5996013\r## 0.0328 0.7731359 0.4023295 0.5996013\r## 0.0329 0.7731359 0.4023295 0.5996013\r## 0.0330 0.7731359 0.4023295 0.5996013\r## 0.0331 0.7731359 0.4023295 0.5996013\r## 0.0332 0.7731359 0.4023295 0.5996013\r## 0.0333 0.7731359 0.4023295 0.5996013\r## 0.0334 0.7731360 0.4023295 0.5996014\r## 0.0335 0.7731363 0.4023295 0.5996018\r## 0.0336 0.7731366 0.4023294 0.5996023\r## 0.0337 0.7731369 0.4023294 0.5996028\r## 0.0338 0.7731372 0.4023293 0.5996033\r## 0.0339 0.7731375 0.4023293 0.5996037\r## 0.0340 0.7731378 0.4023292 0.5996042\r## 0.0341 0.7731381 0.4023292 0.5996047\r## 0.0342 0.7731384 0.4023291 0.5996052\r## 0.0343 0.7731387 0.4023291 0.5996057\r## 0.0344 0.7731390 0.4023290 0.5996062\r## 0.0345 0.7731393 0.4023290 0.5996067\r## 0.0346 0.7731396 0.4023289 0.5996072\r## 0.0347 0.7731399 0.4023289 0.5996077\r## 0.0348 0.7731402 0.4023288 0.5996082\r## 0.0349 0.7731405 0.4023288 0.5996087\r## 0.0350 0.7731408 0.4023287 0.5996092\r## 0.0351 0.7731411 0.4023287 0.5996097\r## 0.0352 0.7731415 0.4023286 0.5996102\r## 0.0353 0.7731418 0.4023286 0.5996107\r## 0.0354 0.7731421 0.4023285 0.5996112\r## 0.0355 0.7731424 0.4023285 0.5996117\r## 0.0356 0.7731427 0.4023284 0.5996122\r## 0.0357 0.7731430 0.4023283 0.5996127\r## 0.0358 0.7731433 0.4023283 0.5996132\r## 0.0359 0.7731437 0.4023282 0.5996137\r## 0.0360 0.7731440 0.4023282 0.5996142\r## 0.0361 0.7731443 0.4023281 0.5996147\r## 0.0362 0.7731446 0.4023281 0.5996152\r## 0.0363 0.7731449 0.4023280 0.5996157\r## 0.0364 0.7731453 0.4023280 0.5996162\r## 0.0365 0.7731456 0.4023279 0.5996167\r## 0.0366 0.7731459 0.4023279 0.5996172\r## 0.0367 0.7731462 0.4023278 0.5996177\r## 0.0368 0.7731465 0.4023278 0.5996182\r## 0.0369 0.7731469 0.4023277 0.5996188\r## 0.0370 0.7731472 0.4023276 0.5996193\r## 0.0371 0.7731475 0.4023276 0.5996198\r## 0.0372 0.7731478 0.4023275 0.5996203\r## 0.0373 0.7731482 0.4023275 0.5996208\r## 0.0374 0.7731485 0.4023274 0.5996213\r## 0.0375 0.7731488 0.4023273 0.5996218\r## 0.0376 0.7731492 0.4023273 0.5996223\r## 0.0377 0.7731495 0.4023272 0.5996228\r## 0.0378 0.7731498 0.4023272 0.5996234\r## 0.0379 0.7731501 0.4023271 0.5996239\r## 0.0380 0.7731505 0.4023271 0.5996244\r## 0.0381 0.7731508 0.4023270 0.5996249\r## 0.0382 0.7731511 0.4023269 0.5996254\r## 0.0383 0.7731515 0.4023269 0.5996259\r## 0.0384 0.7731518 0.4023268 0.5996265\r## 0.0385 0.7731522 0.4023268 0.5996270\r## 0.0386 0.7731525 0.4023267 0.5996275\r## 0.0387 0.7731528 0.4023266 0.5996280\r## 0.0388 0.7731532 0.4023266 0.5996285\r## 0.0389 0.7731535 0.4023265 0.5996291\r## 0.0390 0.7731539 0.4023264 0.5996296\r## 0.0391 0.7731542 0.4023264 0.5996301\r## 0.0392 0.7731545 0.4023263 0.5996306\r## 0.0393 0.7731549 0.4023263 0.5996311\r## 0.0394 0.7731552 0.4023262 0.5996317\r## 0.0395 0.7731556 0.4023261 0.5996322\r## 0.0396 0.7731559 0.4023261 0.5996327\r## 0.0397 0.7731563 0.4023260 0.5996332\r## 0.0398 0.7731566 0.4023260 0.5996338\r## 0.0399 0.7731570 0.4023259 0.5996343\r## 0.0400 0.7731573 0.4023258 0.5996348\r## ## Tuning parameter \u0026#39;alpha\u0026#39; was held constant at a value of 0\r## RMSE was used to select the optimal model using the smallest value.\r## The final values used for the model were alpha = 0 and lambda = 0.0333.\rpredicted_test \u0026lt;- predict(or_mod_2, oregon_test)\rRMSE_test \u0026lt;- RMSE(predicted_test, oregon_test$score)\rcat(\u0026quot;Test RMSEA is\u0026quot;, RMSE_test, \u0026quot;\\n\u0026quot;)\r## Test RMSEA is 2501.218\rrsq_test \u0026lt;- cor(predicted_test, oregon_test$score)^2\rcat(\u0026quot;Test R-squared is\u0026quot;, rsq_test, \u0026quot;\\n\u0026quot;)\r## Test R-squared is 0.4012969\rmae_test \u0026lt;- MAE(predicted_test, oregon_test$score)\rcat(\u0026quot;Test MAE is\u0026quot;, mae_test, \u0026quot;\\n\u0026quot;)\r## Test MAE is 2498.592\rmod_2_stats \u0026lt;- c(\u0026quot;Ridge Regression\u0026quot;, RMSE_test, rsq_test, mae_test)\rNot that I can tell! Error increased from non-regularized model test; also over-fitting data (smaller error values for crossfold training set) I tried following hyperparameter tunings- 1) .01:3 by .1 (Used .01 for parameter) 2) .001:.01 by .0001 (Used .01) 3) .005: .05 by .0005 (Used .033) 4) .003 to .04 by .0001 (Used .0333) RMSEA remained at 2501.218 across models, so stopped testing\nTask 2.5.\rUse the caret::train() function to train a model with 10-fold cross-validation to predict the scores using lasso regression. Try different values of lambda to decide optimal value. Evaluate the performance of the model on the test dataset, and report RMSE, R-square, and MAE. Does lasso regression provide any improvement over linear regression with no regularization?\nlasso_grid \u0026lt;- data.frame(alpha = 1, lambda = seq(0.00079, 0.00083, 0.0000001))\rcross_or \u0026lt;- crossfold(oregon_train, 10)\ror_mod_3 \u0026lt;- caret::train(blueprint_oregon_ridge, data = oregon_train, method = \u0026quot;glmnet\u0026quot;, trControl = cross_or, tuneGrid = lasso_grid)\ror_mod_3\r## glmnet ## ## 151541 samples\r## 29 predictor\r## ## Recipe steps: indicate_na, zv, impute_mean, impute_mode, harmonic,\r## harmonic, ns, normalize, normalize, dummy, rm, normalize ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 136386, 136387, 136387, 136387, 136387, 136387, ... ## Resampling results across tuning parameters:\r## ## lambda RMSE Rsquared MAE ## 0.0007900 0.7731122 0.4023048 0.5995311\r## 0.0007901 0.7731122 0.4023048 0.5995311\r## 0.0007902 0.7731122 0.4023048 0.5995311\r## 0.0007903 0.7731122 0.4023048 0.5995311\r## 0.0007904 0.7731122 0.4023048 0.5995311\r## 0.0007905 0.7731122 0.4023048 0.5995311\r## 0.0007906 0.7731122 0.4023048 0.5995311\r## 0.0007907 0.7731122 0.4023048 0.5995311\r## 0.0007908 0.7731122 0.4023048 0.5995311\r## 0.0007909 0.7731121 0.4023048 0.5995311\r## 0.0007910 0.7731121 0.4023048 0.5995311\r## 0.0007911 0.7731121 0.4023048 0.5995311\r## 0.0007912 0.7731121 0.4023048 0.5995311\r## 0.0007913 0.7731121 0.4023048 0.5995311\r## 0.0007914 0.7731121 0.4023048 0.5995311\r## 0.0007915 0.7731121 0.4023048 0.5995311\r## 0.0007916 0.7731121 0.4023048 0.5995311\r## 0.0007917 0.7731121 0.4023048 0.5995311\r## 0.0007918 0.7731121 0.4023048 0.5995311\r## 0.0007919 0.7731121 0.4023048 0.5995311\r## 0.0007920 0.7731121 0.4023048 0.5995311\r## 0.0007921 0.7731121 0.4023048 0.5995311\r## 0.0007922 0.7731121 0.4023048 0.5995311\r## 0.0007923 0.7731121 0.4023048 0.5995311\r## 0.0007924 0.7731121 0.4023049 0.5995311\r## 0.0007925 0.7731121 0.4023049 0.5995311\r## 0.0007926 0.7731121 0.4023049 0.5995311\r## 0.0007927 0.7731121 0.4023049 0.5995312\r## 0.0007928 0.7731121 0.4023049 0.5995312\r## 0.0007929 0.7731121 0.4023049 0.5995312\r## 0.0007930 0.7731121 0.4023049 0.5995312\r## 0.0007931 0.7731121 0.4023049 0.5995312\r## 0.0007932 0.7731121 0.4023049 0.5995312\r## 0.0007933 0.7731121 0.4023049 0.5995312\r## 0.0007934 0.7731121 0.4023049 0.5995312\r## 0.0007935 0.7731121 0.4023049 0.5995312\r## 0.0007936 0.7731121 0.4023049 0.5995312\r## 0.0007937 0.7731121 0.4023049 0.5995312\r## 0.0007938 0.7731121 0.4023049 0.5995312\r## 0.0007939 0.7731121 0.4023049 0.5995312\r## 0.0007940 0.7731121 0.4023049 0.5995312\r## 0.0007941 0.7731121 0.4023049 0.5995312\r## 0.0007942 0.7731121 0.4023049 0.5995312\r## 0.0007943 0.7731122 0.4023049 0.5995312\r## 0.0007944 0.7731122 0.4023049 0.5995312\r## 0.0007945 0.7731122 0.4023049 0.5995312\r## 0.0007946 0.7731122 0.4023049 0.5995312\r## 0.0007947 0.7731122 0.4023049 0.5995312\r## 0.0007948 0.7731122 0.4023049 0.5995312\r## 0.0007949 0.7731122 0.4023049 0.5995312\r## 0.0007950 0.7731122 0.4023049 0.5995312\r## 0.0007951 0.7731122 0.4023049 0.5995312\r## 0.0007952 0.7731122 0.4023049 0.5995312\r## 0.0007953 0.7731122 0.4023049 0.5995312\r## 0.0007954 0.7731122 0.4023048 0.5995312\r## 0.0007955 0.7731122 0.4023048 0.5995312\r## 0.0007956 0.7731122 0.4023048 0.5995312\r## 0.0007957 0.7731122 0.4023048 0.5995312\r## 0.0007958 0.7731122 0.4023048 0.5995312\r## 0.0007959 0.7731122 0.4023048 0.5995313\r## 0.0007960 0.7731122 0.4023048 0.5995313\r## 0.0007961 0.7731122 0.4023048 0.5995313\r## 0.0007962 0.7731122 0.4023048 0.5995313\r## 0.0007963 0.7731122 0.4023048 0.5995313\r## 0.0007964 0.7731122 0.4023048 0.5995313\r## 0.0007965 0.7731122 0.4023048 0.5995313\r## 0.0007966 0.7731122 0.4023048 0.5995313\r## 0.0007967 0.7731122 0.4023048 0.5995313\r## 0.0007968 0.7731122 0.4023048 0.5995313\r## 0.0007969 0.7731122 0.4023048 0.5995313\r## 0.0007970 0.7731122 0.4023048 0.5995313\r## 0.0007971 0.7731122 0.4023048 0.5995313\r## 0.0007972 0.7731122 0.4023048 0.5995313\r## 0.0007973 0.7731122 0.4023048 0.5995313\r## 0.0007974 0.7731122 0.4023048 0.5995313\r## 0.0007975 0.7731122 0.4023048 0.5995313\r## 0.0007976 0.7731122 0.4023048 0.5995313\r## 0.0007977 0.7731122 0.4023048 0.5995313\r## 0.0007978 0.7731122 0.4023048 0.5995313\r## 0.0007979 0.7731122 0.4023048 0.5995313\r## 0.0007980 0.7731122 0.4023048 0.5995313\r## 0.0007981 0.7731122 0.4023048 0.5995313\r## 0.0007982 0.7731122 0.4023048 0.5995313\r## 0.0007983 0.7731122 0.4023048 0.5995313\r## 0.0007984 0.7731122 0.4023048 0.5995313\r## 0.0007985 0.7731122 0.4023048 0.5995313\r## 0.0007986 0.7731122 0.4023048 0.5995313\r## 0.0007987 0.7731122 0.4023048 0.5995313\r## 0.0007988 0.7731122 0.4023048 0.5995313\r## 0.0007989 0.7731122 0.4023048 0.5995313\r## 0.0007990 0.7731122 0.4023048 0.5995314\r## 0.0007991 0.7731122 0.4023048 0.5995314\r## 0.0007992 0.7731122 0.4023048 0.5995314\r## 0.0007993 0.7731122 0.4023048 0.5995314\r## 0.0007994 0.7731122 0.4023048 0.5995314\r## 0.0007995 0.7731122 0.4023048 0.5995314\r## 0.0007996 0.7731122 0.4023048 0.5995314\r## 0.0007997 0.7731122 0.4023048 0.5995314\r## 0.0007998 0.7731122 0.4023048 0.5995314\r## 0.0007999 0.7731122 0.4023048 0.5995314\r## 0.0008000 0.7731122 0.4023048 0.5995314\r## 0.0008001 0.7731122 0.4023048 0.5995314\r## 0.0008002 0.7731122 0.4023048 0.5995314\r## 0.0008003 0.7731122 0.4023048 0.5995314\r## 0.0008004 0.7731122 0.4023048 0.5995314\r## 0.0008005 0.7731122 0.4023048 0.5995314\r## 0.0008006 0.7731122 0.4023048 0.5995314\r## 0.0008007 0.7731122 0.4023048 0.5995314\r## 0.0008008 0.7731122 0.4023048 0.5995314\r## 0.0008009 0.7731122 0.4023048 0.5995314\r## 0.0008010 0.7731122 0.4023048 0.5995314\r## 0.0008011 0.7731122 0.4023048 0.5995314\r## 0.0008012 0.7731122 0.4023048 0.5995314\r## 0.0008013 0.7731122 0.4023048 0.5995314\r## 0.0008014 0.7731122 0.4023048 0.5995314\r## 0.0008015 0.7731122 0.4023048 0.5995314\r## 0.0008016 0.7731122 0.4023048 0.5995314\r## 0.0008017 0.7731122 0.4023048 0.5995314\r## 0.0008018 0.7731122 0.4023048 0.5995314\r## 0.0008019 0.7731122 0.4023048 0.5995314\r## 0.0008020 0.7731122 0.4023048 0.5995314\r## 0.0008021 0.7731122 0.4023048 0.5995315\r## 0.0008022 0.7731122 0.4023048 0.5995315\r## 0.0008023 0.7731122 0.4023048 0.5995315\r## 0.0008024 0.7731122 0.4023048 0.5995315\r## 0.0008025 0.7731122 0.4023048 0.5995315\r## 0.0008026 0.7731122 0.4023048 0.5995315\r## 0.0008027 0.7731122 0.4023048 0.5995315\r## 0.0008028 0.7731122 0.4023048 0.5995315\r## 0.0008029 0.7731122 0.4023048 0.5995315\r## 0.0008030 0.7731122 0.4023048 0.5995315\r## 0.0008031 0.7731122 0.4023048 0.5995315\r## 0.0008032 0.7731122 0.4023048 0.5995315\r## 0.0008033 0.7731122 0.4023048 0.5995315\r## 0.0008034 0.7731122 0.4023048 0.5995315\r## 0.0008035 0.7731122 0.4023048 0.5995315\r## 0.0008036 0.7731122 0.4023048 0.5995315\r## 0.0008037 0.7731122 0.4023048 0.5995315\r## 0.0008038 0.7731122 0.4023048 0.5995315\r## 0.0008039 0.7731122 0.4023048 0.5995315\r## 0.0008040 0.7731122 0.4023048 0.5995315\r## 0.0008041 0.7731122 0.4023048 0.5995315\r## 0.0008042 0.7731122 0.4023048 0.5995315\r## 0.0008043 0.7731122 0.4023048 0.5995315\r## 0.0008044 0.7731122 0.4023048 0.5995315\r## 0.0008045 0.7731122 0.4023048 0.5995315\r## 0.0008046 0.7731122 0.4023048 0.5995315\r## 0.0008047 0.7731122 0.4023048 0.5995315\r## 0.0008048 0.7731122 0.4023048 0.5995315\r## 0.0008049 0.7731122 0.4023048 0.5995315\r## 0.0008050 0.7731122 0.4023048 0.5995315\r## 0.0008051 0.7731122 0.4023048 0.5995316\r## 0.0008052 0.7731122 0.4023048 0.5995316\r## 0.0008053 0.7731122 0.4023048 0.5995316\r## 0.0008054 0.7731122 0.4023048 0.5995316\r## 0.0008055 0.7731122 0.4023048 0.5995316\r## 0.0008056 0.7731122 0.4023048 0.5995316\r## 0.0008057 0.7731122 0.4023048 0.5995316\r## 0.0008058 0.7731122 0.4023048 0.5995316\r## 0.0008059 0.7731122 0.4023048 0.5995316\r## 0.0008060 0.7731122 0.4023048 0.5995316\r## 0.0008061 0.7731122 0.4023048 0.5995316\r## 0.0008062 0.7731122 0.4023048 0.5995316\r## 0.0008063 0.7731122 0.4023048 0.5995316\r## 0.0008064 0.7731122 0.4023048 0.5995316\r## 0.0008065 0.7731122 0.4023048 0.5995316\r## 0.0008066 0.7731122 0.4023048 0.5995316\r## 0.0008067 0.7731122 0.4023048 0.5995316\r## 0.0008068 0.7731122 0.4023048 0.5995316\r## 0.0008069 0.7731122 0.4023048 0.5995316\r## 0.0008070 0.7731122 0.4023048 0.5995316\r## 0.0008071 0.7731122 0.4023048 0.5995316\r## 0.0008072 0.7731122 0.4023048 0.5995316\r## 0.0008073 0.7731122 0.4023048 0.5995316\r## 0.0008074 0.7731122 0.4023048 0.5995316\r## 0.0008075 0.7731122 0.4023048 0.5995316\r## 0.0008076 0.7731122 0.4023048 0.5995316\r## 0.0008077 0.7731122 0.4023048 0.5995316\r## 0.0008078 0.7731122 0.4023048 0.5995316\r## 0.0008079 0.7731122 0.4023048 0.5995316\r## 0.0008080 0.7731122 0.4023048 0.5995316\r## 0.0008081 0.7731122 0.4023048 0.5995316\r## 0.0008082 0.7731122 0.4023048 0.5995317\r## 0.0008083 0.7731122 0.4023048 0.5995317\r## 0.0008084 0.7731122 0.4023048 0.5995317\r## 0.0008085 0.7731122 0.4023048 0.5995317\r## 0.0008086 0.7731122 0.4023048 0.5995317\r## 0.0008087 0.7731122 0.4023048 0.5995317\r## 0.0008088 0.7731122 0.4023048 0.5995317\r## 0.0008089 0.7731122 0.4023048 0.5995317\r## 0.0008090 0.7731122 0.4023048 0.5995317\r## 0.0008091 0.7731122 0.4023048 0.5995317\r## 0.0008092 0.7731122 0.4023048 0.5995317\r## 0.0008093 0.7731122 0.4023048 0.5995317\r## 0.0008094 0.7731122 0.4023048 0.5995317\r## 0.0008095 0.7731122 0.4023048 0.5995317\r## 0.0008096 0.7731122 0.4023048 0.5995317\r## 0.0008097 0.7731122 0.4023048 0.5995317\r## 0.0008098 0.7731122 0.4023048 0.5995317\r## 0.0008099 0.7731122 0.4023048 0.5995317\r## 0.0008100 0.7731122 0.4023048 0.5995317\r## 0.0008101 0.7731122 0.4023048 0.5995317\r## 0.0008102 0.7731122 0.4023048 0.5995317\r## 0.0008103 0.7731122 0.4023048 0.5995317\r## 0.0008104 0.7731122 0.4023048 0.5995317\r## 0.0008105 0.7731122 0.4023048 0.5995317\r## 0.0008106 0.7731122 0.4023048 0.5995317\r## 0.0008107 0.7731122 0.4023048 0.5995317\r## 0.0008108 0.7731122 0.4023048 0.5995317\r## 0.0008109 0.7731122 0.4023048 0.5995317\r## 0.0008110 0.7731122 0.4023048 0.5995317\r## 0.0008111 0.7731122 0.4023048 0.5995317\r## 0.0008112 0.7731122 0.4023048 0.5995317\r## 0.0008113 0.7731122 0.4023048 0.5995318\r## 0.0008114 0.7731122 0.4023048 0.5995318\r## 0.0008115 0.7731122 0.4023048 0.5995318\r## 0.0008116 0.7731122 0.4023048 0.5995318\r## 0.0008117 0.7731122 0.4023048 0.5995318\r## 0.0008118 0.7731122 0.4023048 0.5995318\r## 0.0008119 0.7731122 0.4023048 0.5995318\r## 0.0008120 0.7731122 0.4023048 0.5995318\r## 0.0008121 0.7731122 0.4023048 0.5995318\r## 0.0008122 0.7731122 0.4023048 0.5995318\r## 0.0008123 0.7731122 0.4023048 0.5995318\r## 0.0008124 0.7731122 0.4023048 0.5995318\r## 0.0008125 0.7731122 0.4023048 0.5995318\r## 0.0008126 0.7731122 0.4023048 0.5995318\r## 0.0008127 0.7731122 0.4023048 0.5995318\r## 0.0008128 0.7731122 0.4023048 0.5995318\r## 0.0008129 0.7731122 0.4023048 0.5995318\r## 0.0008130 0.7731122 0.4023048 0.5995318\r## 0.0008131 0.7731122 0.4023048 0.5995318\r## 0.0008132 0.7731122 0.4023048 0.5995318\r## 0.0008133 0.7731122 0.4023048 0.5995318\r## 0.0008134 0.7731122 0.4023048 0.5995318\r## 0.0008135 0.7731122 0.4023048 0.5995318\r## 0.0008136 0.7731122 0.4023048 0.5995318\r## 0.0008137 0.7731122 0.4023048 0.5995318\r## 0.0008138 0.7731122 0.4023048 0.5995318\r## 0.0008139 0.7731122 0.4023048 0.5995318\r## 0.0008140 0.7731122 0.4023048 0.5995318\r## 0.0008141 0.7731122 0.4023048 0.5995318\r## 0.0008142 0.7731122 0.4023048 0.5995318\r## 0.0008143 0.7731122 0.4023048 0.5995318\r## 0.0008144 0.7731122 0.4023048 0.5995319\r## 0.0008145 0.7731122 0.4023048 0.5995319\r## 0.0008146 0.7731122 0.4023048 0.5995319\r## 0.0008147 0.7731122 0.4023048 0.5995319\r## 0.0008148 0.7731122 0.4023048 0.5995319\r## 0.0008149 0.7731122 0.4023048 0.5995319\r## 0.0008150 0.7731122 0.4023048 0.5995319\r## 0.0008151 0.7731122 0.4023048 0.5995319\r## 0.0008152 0.7731122 0.4023048 0.5995319\r## 0.0008153 0.7731122 0.4023048 0.5995319\r## 0.0008154 0.7731122 0.4023048 0.5995319\r## 0.0008155 0.7731122 0.4023048 0.5995319\r## 0.0008156 0.7731122 0.4023048 0.5995319\r## 0.0008157 0.7731122 0.4023048 0.5995319\r## 0.0008158 0.7731122 0.4023048 0.5995319\r## 0.0008159 0.7731122 0.4023048 0.5995319\r## 0.0008160 0.7731122 0.4023048 0.5995319\r## 0.0008161 0.7731122 0.4023048 0.5995319\r## 0.0008162 0.7731122 0.4023048 0.5995319\r## 0.0008163 0.7731122 0.4023048 0.5995319\r## 0.0008164 0.7731122 0.4023048 0.5995319\r## 0.0008165 0.7731122 0.4023048 0.5995319\r## 0.0008166 0.7731122 0.4023048 0.5995319\r## 0.0008167 0.7731122 0.4023048 0.5995319\r## 0.0008168 0.7731122 0.4023048 0.5995319\r## 0.0008169 0.7731122 0.4023048 0.5995319\r## 0.0008170 0.7731122 0.4023048 0.5995319\r## 0.0008171 0.7731122 0.4023048 0.5995319\r## 0.0008172 0.7731122 0.4023048 0.5995319\r## 0.0008173 0.7731122 0.4023048 0.5995319\r## 0.0008174 0.7731122 0.4023048 0.5995319\r## 0.0008175 0.7731122 0.4023048 0.5995320\r## 0.0008176 0.7731122 0.4023048 0.5995320\r## 0.0008177 0.7731122 0.4023048 0.5995320\r## 0.0008178 0.7731122 0.4023048 0.5995320\r## 0.0008179 0.7731122 0.4023048 0.5995320\r## 0.0008180 0.7731122 0.4023048 0.5995320\r## 0.0008181 0.7731122 0.4023048 0.5995320\r## 0.0008182 0.7731122 0.4023048 0.5995320\r## 0.0008183 0.7731122 0.4023048 0.5995320\r## 0.0008184 0.7731122 0.4023048 0.5995320\r## 0.0008185 0.7731122 0.4023048 0.5995320\r## 0.0008186 0.7731122 0.4023048 0.5995320\r## 0.0008187 0.7731122 0.4023048 0.5995320\r## 0.0008188 0.7731122 0.4023048 0.5995320\r## 0.0008189 0.7731122 0.4023048 0.5995320\r## 0.0008190 0.7731122 0.4023048 0.5995320\r## 0.0008191 0.7731122 0.4023048 0.5995320\r## 0.0008192 0.7731122 0.4023048 0.5995320\r## 0.0008193 0.7731122 0.4023048 0.5995320\r## 0.0008194 0.7731122 0.4023048 0.5995320\r## 0.0008195 0.7731122 0.4023048 0.5995320\r## 0.0008196 0.7731122 0.4023048 0.5995320\r## 0.0008197 0.7731122 0.4023048 0.5995320\r## 0.0008198 0.7731122 0.4023048 0.5995320\r## 0.0008199 0.7731122 0.4023048 0.5995320\r## 0.0008200 0.7731122 0.4023048 0.5995320\r## 0.0008201 0.7731122 0.4023048 0.5995320\r## 0.0008202 0.7731122 0.4023048 0.5995320\r## 0.0008203 0.7731122 0.4023048 0.5995320\r## 0.0008204 0.7731122 0.4023048 0.5995320\r## 0.0008205 0.7731122 0.4023048 0.5995320\r## 0.0008206 0.7731122 0.4023048 0.5995321\r## 0.0008207 0.7731122 0.4023048 0.5995321\r## 0.0008208 0.7731122 0.4023048 0.5995321\r## 0.0008209 0.7731122 0.4023048 0.5995321\r## 0.0008210 0.7731122 0.4023048 0.5995321\r## 0.0008211 0.7731122 0.4023048 0.5995321\r## 0.0008212 0.7731122 0.4023048 0.5995321\r## 0.0008213 0.7731122 0.4023048 0.5995321\r## 0.0008214 0.7731122 0.4023048 0.5995321\r## 0.0008215 0.7731122 0.4023048 0.5995321\r## 0.0008216 0.7731122 0.4023048 0.5995321\r## 0.0008217 0.7731122 0.4023048 0.5995321\r## 0.0008218 0.7731122 0.4023048 0.5995321\r## 0.0008219 0.7731122 0.4023048 0.5995321\r## 0.0008220 0.7731122 0.4023048 0.5995321\r## 0.0008221 0.7731122 0.4023048 0.5995321\r## 0.0008222 0.7731122 0.4023048 0.5995321\r## 0.0008223 0.7731122 0.4023048 0.5995321\r## 0.0008224 0.7731122 0.4023048 0.5995321\r## 0.0008225 0.7731122 0.4023048 0.5995321\r## 0.0008226 0.7731122 0.4023048 0.5995321\r## 0.0008227 0.7731122 0.4023048 0.5995321\r## 0.0008228 0.7731122 0.4023048 0.5995321\r## 0.0008229 0.7731122 0.4023048 0.5995321\r## 0.0008230 0.7731122 0.4023048 0.5995321\r## 0.0008231 0.7731122 0.4023048 0.5995321\r## 0.0008232 0.7731122 0.4023048 0.5995321\r## 0.0008233 0.7731122 0.4023048 0.5995321\r## 0.0008234 0.7731122 0.4023048 0.5995321\r## 0.0008235 0.7731122 0.4023048 0.5995321\r## 0.0008236 0.7731122 0.4023048 0.5995321\r## 0.0008237 0.7731122 0.4023048 0.5995322\r## 0.0008238 0.7731122 0.4023048 0.5995322\r## 0.0008239 0.7731122 0.4023048 0.5995322\r## 0.0008240 0.7731122 0.4023048 0.5995322\r## 0.0008241 0.7731122 0.4023048 0.5995322\r## 0.0008242 0.7731122 0.4023048 0.5995322\r## 0.0008243 0.7731122 0.4023048 0.5995322\r## 0.0008244 0.7731122 0.4023048 0.5995322\r## 0.0008245 0.7731122 0.4023048 0.5995322\r## 0.0008246 0.7731122 0.4023048 0.5995322\r## 0.0008247 0.7731122 0.4023048 0.5995322\r## 0.0008248 0.7731122 0.4023048 0.5995322\r## 0.0008249 0.7731122 0.4023048 0.5995322\r## 0.0008250 0.7731122 0.4023048 0.5995322\r## 0.0008251 0.7731122 0.4023048 0.5995322\r## 0.0008252 0.7731122 0.4023048 0.5995322\r## 0.0008253 0.7731122 0.4023048 0.5995322\r## 0.0008254 0.7731122 0.4023048 0.5995322\r## 0.0008255 0.7731122 0.4023048 0.5995322\r## 0.0008256 0.7731122 0.4023048 0.5995322\r## 0.0008257 0.7731122 0.4023048 0.5995322\r## 0.0008258 0.7731122 0.4023048 0.5995322\r## 0.0008259 0.7731122 0.4023048 0.5995322\r## 0.0008260 0.7731122 0.4023048 0.5995322\r## 0.0008261 0.7731122 0.4023048 0.5995322\r## 0.0008262 0.7731122 0.4023048 0.5995322\r## 0.0008263 0.7731122 0.4023048 0.5995322\r## 0.0008264 0.7731122 0.4023048 0.5995322\r## 0.0008265 0.7731122 0.4023048 0.5995322\r## 0.0008266 0.7731122 0.4023048 0.5995322\r## 0.0008267 0.7731122 0.4023048 0.5995322\r## 0.0008268 0.7731123 0.4023048 0.5995323\r## 0.0008269 0.7731123 0.4023048 0.5995323\r## 0.0008270 0.7731123 0.4023048 0.5995323\r## 0.0008271 0.7731123 0.4023048 0.5995323\r## 0.0008272 0.7731123 0.4023048 0.5995323\r## 0.0008273 0.7731123 0.4023048 0.5995323\r## 0.0008274 0.7731123 0.4023048 0.5995323\r## 0.0008275 0.7731123 0.4023048 0.5995323\r## 0.0008276 0.7731123 0.4023048 0.5995323\r## 0.0008277 0.7731123 0.4023048 0.5995323\r## 0.0008278 0.7731123 0.4023048 0.5995323\r## 0.0008279 0.7731123 0.4023048 0.5995323\r## 0.0008280 0.7731123 0.4023048 0.5995323\r## 0.0008281 0.7731123 0.4023048 0.5995323\r## 0.0008282 0.7731123 0.4023048 0.5995323\r## 0.0008283 0.7731123 0.4023048 0.5995323\r## 0.0008284 0.7731123 0.4023048 0.5995323\r## 0.0008285 0.7731123 0.4023048 0.5995323\r## 0.0008286 0.7731123 0.4023048 0.5995323\r## 0.0008287 0.7731123 0.4023048 0.5995323\r## 0.0008288 0.7731123 0.4023048 0.5995323\r## 0.0008289 0.7731123 0.4023048 0.5995323\r## 0.0008290 0.7731123 0.4023048 0.5995323\r## 0.0008291 0.7731123 0.4023048 0.5995323\r## 0.0008292 0.7731123 0.4023048 0.5995323\r## 0.0008293 0.7731123 0.4023048 0.5995323\r## 0.0008294 0.7731123 0.4023048 0.5995323\r## 0.0008295 0.7731123 0.4023048 0.5995323\r## 0.0008296 0.7731123 0.4023048 0.5995323\r## 0.0008297 0.7731123 0.4023048 0.5995323\r## 0.0008298 0.7731123 0.4023048 0.5995323\r## 0.0008299 0.7731123 0.4023048 0.5995324\r## 0.0008300 0.7731123 0.4023048 0.5995324\r## ## Tuning parameter \u0026#39;alpha\u0026#39; was held constant at a value of 1\r## RMSE was used to select the optimal model using the smallest value.\r## The final values used for the model were alpha = 1 and lambda = 0.0007916.\rpredicted_test \u0026lt;- predict(or_mod_3, oregon_test)\rRMSE_test \u0026lt;- RMSE(predicted_test, oregon_test$score)\rcat(\u0026quot;Test RMSEA is\u0026quot;, RMSE_test, \u0026quot;\\n\u0026quot;)\r## Test RMSEA is 2501.218\rrsq_test \u0026lt;- cor(predicted_test, oregon_test$score)^2\rcat(\u0026quot;Test R-squared is\u0026quot;, rsq_test, \u0026quot;\\n\u0026quot;)\r## Test R-squared is 0.4011952\rmae_test \u0026lt;- MAE(predicted_test, oregon_test$score)\rcat(\u0026quot;Test MAE is\u0026quot;, mae_test, \u0026quot;\\n\u0026quot;)\r## Test MAE is 2498.592\rmod_3_stats \u0026lt;- c(\u0026quot;Lasso Regression\u0026quot;, RMSE_test, rsq_test, mae_test)\rI tried the following parameters - 1) .001:3 by .01 (Used .001 for parameter) 2) .00002:.02 by 0001 (Used 0.00082) 3) 0.000005:.01 by .00005 (Used 0.000805) 4) 0.00079:0.00083 by 0.0000001 (Used 0.0007916)\nStopped testing due to negligible improvements in RMSEA/MAE. No improvement; more variability with test dataset.\nTask 2.6\rEvaluate the performance of the models in 2.2, 2.3, and 2.4 on the test dataset. Calculate and report the root mean squared error (RMSE), mean absolute error (MAE), and R-square. Summarize these numbers in a table like the following. Decide and comment on which model you would use to predict scores.\nnames \u0026lt;- c(\u0026quot;Regression Model\u0026quot;, \u0026quot;RMSE\u0026quot;, \u0026quot;R-Squared\u0026quot;, \u0026quot;MAE\u0026quot;)\rrbind(names, mod_1_stats, mod_2_stats, mod_3_stats)\r## [,1] [,2] [,3] ## names \u0026quot;Regression Model\u0026quot; \u0026quot;RMSE\u0026quot; \u0026quot;R-Squared\u0026quot; ## mod_1_stats \u0026quot;Unregularized Regression\u0026quot; \u0026quot;88.9664303339126\u0026quot; \u0026quot;0.401389773008871\u0026quot;\r## mod_2_stats \u0026quot;Ridge Regression\u0026quot; \u0026quot;2501.21841380854\u0026quot; \u0026quot;0.401296921657589\u0026quot;\r## mod_3_stats \u0026quot;Lasso Regression\u0026quot; \u0026quot;2501.21830079616\u0026quot; \u0026quot;0.401195181145662\u0026quot;\r## [,4] ## names \u0026quot;MAE\u0026quot; ## mod_1_stats \u0026quot;69.1845396357428\u0026quot;\r## mod_2_stats \u0026quot;2498.59224564397\u0026quot;\r## mod_3_stats \u0026quot;2498.59229921243\u0026quot;\rIt seems unregularized model performs best, as it has least error.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0d988e3f64895c99e382f1931ff1addb","permalink":"/code/ml-class/hw2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/code/ml-class/hw2/","section":"code","summary":"\u003c!DOCTYPE html\u003e\rHW2\rHW2\rLiz G\r11/14/2021\rPart 1: Logistic Regression\rLoading and Prepping Data\r# Load the following packages needed for modeling in this assignment\rrequire(caret)\r## Loading required package: caret\r## Loading required package: ggplot2\r## Loading required package: lattice\rrequire(recipes)\r## Loading required package: recipes\r## Loading required package: dplyr\r## ## Attaching package: \u0026#39;dplyr\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## filter, lag\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## intersect, setdiff, setequal, union\r## ## Attaching package: \u0026#39;recipes\u0026#39;\r## The following object is masked from \u0026#39;package:stats\u0026#39;:\r## ## step\rrequire(finalfit)\r## Loading required package: finalfit\rrequire(glmnet)\r## Loading required package: glmnet\r## Loading required package: Matrix\r## Loaded glmnet 4.","tags":null,"title":"","type":"code"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e\rHW3\rHW3\rLiz G\r12/4/2021\rknitr::opts_chunk$set(cache =TRUE)\rLoad Packages\rrequire(caret)\r## Loading required package: caret\r## Loading required package: ggplot2\r## Loading required package: lattice\rrequire(recipes)\r## Loading required package: recipes\r## Loading required package: dplyr\r## ## Attaching package: \u0026#39;dplyr\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## filter, lag\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## intersect, setdiff, setequal, union\r## ## Attaching package: \u0026#39;recipes\u0026#39;\r## The following object is masked from \u0026#39;package:stats\u0026#39;:\r## ## step\rrequire(ranger)\r## Loading required package: ranger\rrequire(tidyverse)\r## Loading required package: tidyverse\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v tibble 3.1.5 v purrr 0.3.4\r## v tidyr 1.1.4 v stringr 1.4.0\r## v readr 2.0.2 v forcats 0.5.1\r## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x stringr::fixed() masks recipes::fixed()\r## x dplyr::lag() masks stats::lag()\r## x purrr::lift() masks caret::lift()\rrequire(ModelMetrics)\r## Loading required package: ModelMetrics\r## ## Attaching package: \u0026#39;ModelMetrics\u0026#39;\r## The following objects are masked from \u0026#39;package:caret\u0026#39;:\r## ## confusionMatrix, precision, recall, sensitivity, specificity\r## The following object is masked from \u0026#39;package:base\u0026#39;:\r## ## kappa\rTask 1\rImport and Blueprint\r# Import the tweet dataset with embeddings\rtweet \u0026lt;- read.csv(\u0026#39;https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/hw1_tweet_final.csv\u0026#39;,header=TRUE)\r# Recipe for the tweet dataset\rblueprint_tweet \u0026lt;- recipe(x = tweet,\rvars = colnames(tweet),\rroles = c(\u0026#39;outcome\u0026#39;,rep(\u0026#39;predictor\u0026#39;,772))) %\u0026gt;%\rstep_dummy(\u0026#39;month\u0026#39;,one_hot=TRUE) %\u0026gt;% step_harmonic(\u0026#39;day\u0026#39;,frequency=1,cycle_size=7, role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_harmonic(\u0026#39;date\u0026#39;,frequency=1,cycle_size=31,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_harmonic(\u0026#39;hour\u0026#39;,frequency=1,cycle_size=24,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_normalize(paste0(\u0026#39;Dim\u0026#39;,1:768)) %\u0026gt;%\rstep_normalize(c(\u0026#39;day_sin_1\u0026#39;,\u0026#39;day_cos_1\u0026#39;,\r\u0026#39;date_sin_1\u0026#39;,\u0026#39;date_cos_1\u0026#39;,\r\u0026#39;hour_sin_1\u0026#39;,\u0026#39;hour_cos_1\u0026#39;)) %\u0026gt;%\rstep_rm(c(\u0026#39;day\u0026#39;,\u0026#39;date\u0026#39;,\u0026#39;hour\u0026#39;)) %\u0026gt;%\rstep_num2factor(sentiment,\rtransform = function(x) x + 1,\rlevels=c(\u0026#39;Negative\u0026#39;,\u0026#39;Positive\u0026#39;))\rMaking Functions\r##Making a crossfold function\rcrossfold_log \u0026lt;- function(training_data, folds){\r#shuffle data\rtraning_data \u0026lt;- training_data[sample(nrow(training_data)),]\r# Create 10 folds with equal size\rN_folds = cut(seq(1,nrow(training_data)),breaks= folds,labels=FALSE)\r# Create the list for each fold my.indices \u0026lt;- vector(\u0026#39;list\u0026#39;,folds)\rfor(i in 1:folds){\rmy.indices[[i]] \u0026lt;- which(N_folds!=i)\r}\r#cross validation settings\rcv \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;,\rindex = my.indices,\rclassProbs = TRUE,\rsummaryFunction = mnLogLoss)\rreturn(cv)\r}\r#Making an accuracy function accuracy \u0026lt;- function(observed_vector, predicted_vector){\rtab \u0026lt;- table(predicted_vector,\robserved_vector,\rdnn = c(\u0026#39;Predicted\u0026#39;,\u0026#39;Observed\u0026#39;))\rtn \u0026lt;- tab[1,1]\rtp \u0026lt;- tab[2,2]\rfp \u0026lt;- tab[2,1]\rfn \u0026lt;- tab[1,2]\racc \u0026lt;- (tp + tn)/(tp+tn+fp+fn)\rreturn(acc)\r}\rTask 1.1\rSplit the original data into two subsets: training and test. Let the training data have the 80% of cases and the test data have the 20% of the cases.\nset.seed(12142021) # for reproducibility\rloc \u0026lt;- sample(1:nrow(tweet), round(nrow(tweet) * 0.8))\rtweet_train \u0026lt;- tweet[loc, ]\rtweet_test \u0026lt;- tweet[-loc, ]\rprepare \u0026lt;- prep(blueprint_tweet, training = tweet_train)\rbaked_train \u0026lt;- bake(prepare, new_data = tweet_train)\rbaked_test \u0026lt;- bake(prepare, new_data = tweet_test)\rTask 1.2.\rUse the caret::train() function and ranger engine to train a model with 10-fold cross-validation for predicting the probability of sentiment being positive using a Bagged Trees model with 500 trees.\ncrossfold_tweet \u0026lt;- crossfold_log(tweet_train, 10)\rgrid \u0026lt;- expand.grid(mtry = 773,\rsplitrule=\u0026#39;gini\u0026#39;,\rmin.node.size=2)\rmod_1 \u0026lt;- caret::train(blueprint_tweet,\rdata = tweet_train,\rmethod = \u0026#39;ranger\u0026#39;,\rtrControl = crossfold_tweet,\rtuneGrid = grid,\rmetric = \u0026#39;logLoss\u0026#39;,\rnum.trees = 500,\rmax.depth = 60)\rmod_1 \u0026lt;- readRDS(\u0026quot;mod_1.RData\u0026quot;)\rpredicted_test \u0026lt;- predict(mod_1, tweet_test) %\u0026gt;% as.numeric() %\u0026gt;% -1 #subtracting 1 b/c factor levels came out as 1/2 rather than 0/1\r#feel free to provide feedback if you know why this ^^ is...\robserved_test \u0026lt;- tweet_test$sentiment %\u0026gt;% as.numeric()\r##Eval Metrics\r#LogLoss\rLL \u0026lt;- logLoss(observed_test, predicted_test)\r#AUC\rAUC \u0026lt;- auc(observed_test, predicted_test)\r#Accuracy\rACC \u0026lt;- accuracy(observed_test, predicted_test)\r#True Positive Rate\rTPR \u0026lt;- tpr(observed_test, predicted_test, cutoff = .5)\r#True Negative Rate\rTNR \u0026lt;- tnr(observed_test, predicted_test, cutoff = .5)\r#Precision\rPRE \u0026lt;- precision(observed_test, predicted_test, cutoff = .5)\rmod_1_stats \u0026lt;- c(\u0026quot;Bagged Trees\u0026quot;, LL, AUC, TPR, TNR, PRE)\rTask 1.3.\rUse the caret::train() function and ranger engine to train a model with 10-fold cross-validation for predicting the probability of sentiment being positive using a Random Forest model with 500 trees. Set the number of predictors to consider to 250 for each tree while growing a random forest.\ncrossfold_tweet \u0026lt;- crossfold_log(tweet_train, 10)\r# Grid settings\rgrid \u0026lt;- expand.grid(mtry = 250,\rsplitrule=\u0026#39;gini\u0026#39;,\rmin.node.size=2)\rmod_2 \u0026lt;- caret::train(blueprint_tweet,\rdata = tweet_train,\rmethod = \u0026#39;ranger\u0026#39;,\rtrControl = crossfold_tweet,\rtuneGrid = grid,\rnum.trees = 500,\rmax.depth = 60)\rmod_2\u0026lt;- readRDS(\u0026quot;mod_2.RData\u0026quot;)\rpredicted_test \u0026lt;- predict(mod_2, tweet_test) %\u0026gt;% as.numeric() %\u0026gt;% -1 #subtracting 1 b/c factor levels came out as 1/2 rather than 0/1\r#feel free to provide feedback if you know why this ^^ is...\robserved_test \u0026lt;- tweet_test$sentiment %\u0026gt;% as.numeric()\r##Eval Metrics\r#LogLoss\rLL \u0026lt;- logLoss(observed_test, predicted_test)\r#AUC\rAUC \u0026lt;- auc(observed_test, predicted_test)\r#Accuracy\rACC \u0026lt;- accuracy(observed_test, predicted_test)\r#True Positive Rate\rTPR \u0026lt;- tpr(observed_test, predicted_test, cutoff = .5)\r#True Negative Rate\rTNR \u0026lt;- tnr(observed_test, predicted_test, cutoff = .5)\r#Precision\rPRE \u0026lt;- precision(observed_test, predicted_test, cutoff = .5)\rmod_2_stats \u0026lt;- c(\u0026quot;Random Forest\u0026quot;, LL, AUC, TPR, TNR, PRE)\rTask 1.4\rEvaluate the performance of the Bagged Tree models (1.2) and Random Forest Model (1.3) on the test dataset. Calculate and report logLoss (LL), area under the reciever operating characteristic curve (AUC), overall accuracy (ACC), true positive rate (TPR), true negative rate (TNR), and precision (PRE) for three models. When calculating ACC, TPR, TNR, and PRE, assume that we use a cut-off value of 0.5 for the predicted probabilities. Summarize these numbers in a table like the following. Decide and comment on which model you would use to predict sentiment of a tweet moving forward.\nLL AUC ACC TPR TNR PRE Bagged Trees Model\nRandom Forest Model\nnames \u0026lt;- c(\u0026quot;model\u0026quot;, \u0026quot;LL\u0026quot;, \u0026quot;AUC\u0026quot;, \u0026quot;ACC\u0026quot;, \u0026quot;TNR\u0026quot;, \u0026quot;PRE\u0026quot;)\rknitr::kable(rbind(names, mod_1_stats, mod_2_stats))\rnames\rmodel\rLL\rAUC\rACC\rTNR\rPRE\rmod_1_stats\rBagged Trees\r8.74995395429097\r0.749543450180393\r0.811188811188811\r0.687898089171974\r0.703030303030303\rmod_2_stats\rRandom Forest\r8.40455019839326\r0.758162219945659\r0.79020979020979\r0.726114649681529\r0.724358974358974\rI would use the Random Forest Model. While there is a tradeoff w/ accuracy, there is overall higher precision and true negative value, which could be valuable, in terms of the predictive power of the model.\nTask 1.5\rCompare the performance of the Bagged Trees Model and Random Forest Model in this assignment to the performance of logistic regression models from the previous assignment. Comment on what you observe. Did Bagged Trees or Random Forest Models perform better than Logistic Regression Models?\nmod_1_reg \u0026lt;- c(\u0026quot;Unregularized Regression\u0026quot;, \u0026quot;10.3617528580877\u0026quot;, \u0026quot;0.699345240746515\u0026quot;, \u0026quot;0.685314685314685\u0026quot;, \u0026quot;0.713375796178344\u0026quot;, \u0026quot;0.685314685314685\u0026quot;)\rmod_2_reg \u0026lt;- c(\u0026quot;Ridge Regression\u0026quot;, \u0026quot;6.56244747477605\u0026quot;, \u0026quot;0.81005300432052\u0026quot;, \u0026quot;0.811188811188811\u0026quot;,\u0026quot;0.808917197452229\u0026quot;, \u0026quot;0.794520547945205\u0026quot;)\rmod_3_reg \u0026lt;- c(\u0026quot;Lasso Regression\u0026quot;, \u0026quot;6.79269798810115\u0026quot;, \u0026quot;0.802124626965391\u0026quot;,\r\u0026quot;0.776223776223776\u0026quot;, \u0026quot;0.828025477707006\u0026quot;, \u0026quot;0.804347826086957\u0026quot;)\rall_log_mods \u0026lt;- rbind(names, mod_1_stats, mod_2_stats, mod_1_reg, mod_2_reg, mod_3_reg) knitr::kable(all_log_mods)\rnames\rmodel\rLL\rAUC\rACC\rTNR\rPRE\rmod_1_stats\rBagged Trees\r8.74995395429097\r0.749543450180393\r0.811188811188811\r0.687898089171974\r0.703030303030303\rmod_2_stats\rRandom Forest\r8.40455019839326\r0.758162219945659\r0.79020979020979\r0.726114649681529\r0.724358974358974\rmod_1_reg\rUnregularized Regression\r10.3617528580877\r0.699345240746515\r0.685314685314685\r0.713375796178344\r0.685314685314685\rmod_2_reg\rRidge Regression\r6.56244747477605\r0.81005300432052\r0.811188811188811\r0.808917197452229\r0.794520547945205\rmod_3_reg\rLasso Regression\r6.79269798810115\r0.802124626965391\r0.776223776223776\r0.828025477707006\r0.804347826086957\rIt seems ridge regression outperforms random forest models in terms of accuracy, and TNR/precision values (though their performance is most comparable). It seems the only way the tree models outperformed regression models is for bagged trees in terms of model accuracy.\nTask 2\rData Import and Prep\r# Import the oregon dataset\roregon \u0026lt;- read.csv(\u0026#39;https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/hw1_oregon_final.csv\u0026#39;,header=TRUE)\r# Recipe for the oregon dataset\routcome \u0026lt;- \u0026#39;score\u0026#39;\rid \u0026lt;- \u0026#39;id\u0026#39;\rcategorical \u0026lt;- c(\u0026#39;sex\u0026#39;,\u0026#39;ethnic_cd\u0026#39;,\u0026#39;tst_bnch\u0026#39;,\u0026#39;migrant_ed_fg\u0026#39;,\u0026#39;ind_ed_fg\u0026#39;,\r\u0026#39;sp_ed_fg\u0026#39;,\u0026#39;tag_ed_fg\u0026#39;,\u0026#39;econ_dsvntg\u0026#39;,\u0026#39;stay_in_dist\u0026#39;,\r\u0026#39;stay_in_schl\u0026#39;,\u0026#39;dist_sped\u0026#39;,\u0026#39;trgt_assist_fg\u0026#39;,\r\u0026#39;ayp_dist_partic\u0026#39;,\u0026#39;ayp_schl_partic\u0026#39;,\u0026#39;ayp_dist_prfrm\u0026#39;,\r\u0026#39;ayp_schl_prfrm\u0026#39;,\u0026#39;rc_dist_partic\u0026#39;,\u0026#39;rc_schl_partic\u0026#39;,\r\u0026#39;rc_dist_prfrm\u0026#39;,\u0026#39;rc_schl_prfrm\u0026#39;,\u0026#39;grp_rpt_dist_partic\u0026#39;,\r\u0026#39;grp_rpt_schl_partic\u0026#39;,\u0026#39;grp_rpt_dist_prfrm\u0026#39;,\r\u0026#39;grp_rpt_schl_prfrm\u0026#39;)\rnumeric \u0026lt;- c(\u0026#39;enrl_grd\u0026#39;)\rcyclic \u0026lt;- c(\u0026#39;date\u0026#39;,\u0026#39;month\u0026#39;)\rblueprint_oregon \u0026lt;- recipe(x = oregon,\rvars = c(outcome,categorical,numeric,cyclic),\rroles = c(\u0026#39;outcome\u0026#39;,rep(\u0026#39;predictor\u0026#39;,27))) %\u0026gt;%\rstep_indicate_na(all_of(categorical),all_of(numeric)) %\u0026gt;%\rstep_zv(all_numeric()) %\u0026gt;%\rstep_impute_mean(all_of(numeric)) %\u0026gt;%\rstep_impute_mode(all_of(categorical)) %\u0026gt;%\rstep_harmonic(\u0026#39;date\u0026#39;,frequency=1,cycle_size=31,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_harmonic(\u0026#39;month\u0026#39;,frequency=1,cycle_size=12,role=\u0026#39;predictor\u0026#39;) %\u0026gt;%\rstep_ns(\u0026#39;enrl_grd\u0026#39;,deg_free=3) %\u0026gt;%\rstep_normalize(c(paste0(numeric,\u0026#39;_ns_1\u0026#39;),paste0(numeric,\u0026#39;_ns_2\u0026#39;),paste0(numeric,\u0026#39;_ns_3\u0026#39;))) %\u0026gt;%\rstep_normalize(c(\u0026quot;date_sin_1\u0026quot;,\u0026quot;date_cos_1\u0026quot;,\u0026quot;month_sin_1\u0026quot;,\u0026quot;month_cos_1\u0026quot;)) %\u0026gt;%\rstep_dummy(all_of(categorical),one_hot=TRUE) %\u0026gt;%\rstep_rm(c(\u0026#39;date\u0026#39;,\u0026#39;month\u0026#39;))\rMaking Functions\r#making a crossfold training function\rcrossfold \u0026lt;- function(training_data, folds){\r#randomly shuffle data\r# Randomly shuffle the data\rtraining_data = training_data[sample(nrow(training_data)),]\r# Create 10 folds with equal size\rN_folds = cut(seq(1,nrow(training_data)),breaks= folds,labels=FALSE)\r# Create the list for each fold my.indices \u0026lt;- vector(\u0026#39;list\u0026#39;,folds)\rfor(i in 1:folds){\rmy.indices[[i]] \u0026lt;- which(N_folds!=i)\r}\rcv \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;,\rindex = my.indices)\rcv\r}\rTask 2.1.\rSplit the original data into two subsets: training and test. Let the training data have the 80% of cases and the test data have the 20% of the cases.\nset.seed(12042021) # for reproducibility\rloc \u0026lt;- sample(1:nrow(oregon), round(nrow(oregon) * 0.8))\roregon_train \u0026lt;- oregon[loc, ]\roregon_test \u0026lt;- oregon[-loc, ]\rTask 2.2.\rUse the caret::train() function and ranger engine to train a model with 10-fold cross-validation for predicting the scores using a Bagged Trees model with 500 trees.\ncrossfold_or \u0026lt;- crossfold(oregon_train, 10)\rgrid \u0026lt;- expand.grid(mtry = 29,splitrule=\u0026#39;variance\u0026#39;,min.node.size=2)\ror_mod_1 \u0026lt;- caret::train(blueprint_oregon,\rdata = oregon_train,\rmethod = \u0026#39;ranger\u0026#39;,\rtrControl = crossfold_or,\rtuneGrid = grid,\rnum.trees = 500,\rmax.depth = 60)\ror_mod_1\u0026lt;- readRDS(\u0026quot;or_mod_1.RData\u0026quot;)\rpredicted_test \u0026lt;- predict(or_mod_1, oregon_test)\rRMSE_test \u0026lt;- RMSE(predicted_test, oregon_test$score)\rrsq_test \u0026lt;- cor(predicted_test, oregon_test$score)^2\rmae_test \u0026lt;- MAE(predicted_test, oregon_test$score)\rmod_1_stats \u0026lt;- c(\u0026quot;Bagged Trees\u0026quot;, RMSE_test, rsq_test, mae_test)\rsaveRDS(or_mod_1, \u0026quot;ormod.RData\u0026quot;)\rTask 2.3.\rUse the caret::train() function and ranger engine to train a model with 10-fold cross-validation for predicting the scores using a Random Forest model with 500 trees. Set the number of predictors to consider to 25 for each tree while growing a random forest.\ncrossfold_or \u0026lt;- crossfold(oregon_train, 10)\rgrid \u0026lt;- expand.grid(mtry = 25,splitrule=\u0026#39;variance\u0026#39;,min.node.size=2)\ror_mod_2 \u0026lt;- caret::train(blueprint_oregon,\rdata = oregon_train,\rmethod = \u0026#39;ranger\u0026#39;,\rtrControl = crossfold_or,\rtuneGrid = grid,\rnum.trees = 500,\rmax.depth = 60)\ror_mod_2\u0026lt;- readRDS(\u0026quot;or_mod_2.RData\u0026quot;)\rpredicted_test \u0026lt;- predict(or_mod_2, oregon_test)\rRMSE_test \u0026lt;- RMSE(predicted_test, oregon_test$score)\rrsq_test \u0026lt;- cor(predicted_test, oregon_test$score)^2\rmae_test \u0026lt;- MAE(predicted_test, oregon_test$score)\rmod_2_stats \u0026lt;- c(\u0026quot;Random Forest\u0026quot;, RMSE_test, rsq_test, mae_test)\rsaveRDS(or_mod_2, \u0026quot;or_ranforest.RData\u0026quot;)\rTask 2.4\rEvaluate the performance of the Bagged Tree models (2.2) and Random Forest Model (2.3) on the test dataset. Calculate and report the root mean squared error (RMSE), mean absolute error (MAE), and R-square. Summarize these numbers in a table like the following.\nRMSE MAE R-sq Bagged Trees Model\nRandom Forest Model\nnames \u0026lt;- c(\u0026quot;model\u0026quot;, \u0026quot;RMSE\u0026quot;, \u0026quot;R-sq\u0026quot;, \u0026quot;MAE\u0026quot;)\rknitr::kable(rbind(names, mod_1_stats, mod_2_stats))\rnames\rmodel\rRMSE\rR-sq\rMAE\rmod_1_stats\rBagged Trees\r91.0508691477878\r0.391602286095515\r70.4181070651918\rmod_2_stats\rRandom Forest\r90.6718316798684\r0.396013824756484\r70.1278575311851\rIt seems random forest did best in maximizing r-squared (variance predicted) and minimizing error variance\nTask 2.5\rCompare the performance of the Bagged Trees Model and Random Forest Model in this assignment to the performance of linear regression models from the previous assignment. Comment on what you observe. Did Bagged Trees or Random Forest Models perform better than Linear Regression Models in predicting the test scores?\nmod_1_reg \u0026lt;- c(\u0026quot;Unregularized Regression\u0026quot;, \u0026quot;88.9664303339126\u0026quot;, \u0026quot;0.401389773008871\u0026quot;, \u0026quot;69.1845396357428\u0026quot;)\rmod_2_reg \u0026lt;- c(\u0026quot;Ridge Regression\u0026quot;, \u0026quot;2501.21841380854\u0026quot;, \u0026quot;0.401296921657589\u0026quot;, \u0026quot;2498.59224564397\u0026quot;)\rmod_3_reg \u0026lt;- c(\u0026quot;Lasso Regression\u0026quot;, \u0026quot;2501.21830079616\u0026quot;, \u0026quot;0.401195181145662\u0026quot;, \u0026quot;2498.59229921243\u0026quot;)\rall_lin_mods \u0026lt;- rbind(names, mod_1_stats, mod_2_stats, mod_1_reg, mod_2_reg, mod_3_reg)\rknitr::kable(all_lin_mods)\rnames\rmodel\rRMSE\rR-sq\rMAE\rmod_1_stats\rBagged Trees\r91.0508691477878\r0.391602286095515\r70.4181070651918\rmod_2_stats\rRandom Forest\r90.6718316798684\r0.396013824756484\r70.1278575311851\rmod_1_reg\rUnregularized Regression\r88.9664303339126\r0.401389773008871\r69.1845396357428\rmod_2_reg\rRidge Regression\r2501.21841380854\r0.401296921657589\r2498.59224564397\rmod_3_reg\rLasso Regression\r2501.21830079616\r0.401195181145662\r2498.59229921243\rIt seems like Random trees does the best at maximizing the R-sq and minimizing error variance. (Though it seems like something is off w/ my ridge regression and lasso regression error performance stats from HW2…)\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c21abbf5ce77afe3347372dbe1690412","permalink":"/code/ml-class/hw3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/code/ml-class/hw3/","section":"code","summary":"\u003c!DOCTYPE html\u003e\rHW3\rHW3\rLiz G\r12/4/2021\rknitr::opts_chunk$set(cache =TRUE)\rLoad Packages\rrequire(caret)\r## Loading required package: caret\r## Loading required package: ggplot2\r## Loading required package: lattice\rrequire(recipes)\r## Loading required package: recipes\r## Loading required package: dplyr\r## ## Attaching package: \u0026#39;dplyr\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## filter, lag\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## intersect, setdiff, setequal, union\r## ## Attaching package: \u0026#39;recipes\u0026#39;\r## The following object is masked from \u0026#39;package:stats\u0026#39;:\r## ## step\rrequire(ranger)\r## Loading required package: ranger\rrequire(tidyverse)\r## Loading required package: tidyverse\r## -- Attaching packages --------------------------------------- tidyverse 1.","tags":null,"title":"","type":"code"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e\rTranscript Processing Template\rTranscript Processing Template\rLiz G\r9/16/2020\rInstalling R and R Studio\rTo use these functions, you need to install R. You may also want to install the free version of RStudio, as it it a little more user-friendly.\nR is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. https://cran.rstudio.com/\nRStudio is a set of integrated tools designed to help you be more productive with R. It includes a console, syntax-highlighting editor that supports direct code execution, and a variety of robust tools for plotting, viewing history, debugging and managing your workspace. https://rstudio.com/products/rstudio/download/#download\nMake sure you’ve installed the required R packages\rIf you’ve never used R before, or haven’t installed the following packages, you’ll need them to run the function\ninstall.packages(c(\u0026quot;tidyverse\u0026quot;, \u0026quot;here\u0026quot;, \u0026quot;readr\u0026quot;, \u0026quot;rio\u0026quot;))\rRun Program\ryou want to have the file “paragraph_transcript.R” in the same file where you’re running the script\nYou’ll also want to set the input filename (the name of the transcript in a .txt format), and the output file name (what you want the output file to be called). You also want to set the “path” to where the input and output files will be. here can be a\nsource(\u0026quot;paragraph_transcript.R\u0026quot;)\rinput_filename \u0026lt;- \u0026quot;your transcript.txt\u0026quot;\routput_filename \u0026lt;- \u0026quot;your transcript_processed.txt\u0026quot;\rpath \u0026lt;- \u0026quot;yourfilepath\u0026quot;\rsetwd(path)\rparagraph_transcript(path, input_filename, output_filename)\rError messages\rThere will be an error message if the speaker is missing that says “Check row(s) N and check name”.\nYou should add the name using a Speaker: text format\nOnce you’ve made those edits, you can re-run the code\nOtherwise it will say [1] “No errors”, and you should see your output file in the specified computer directory.\nMaking your transcript look great\rYayy! You can use your text processor of choice now (e.g. MS Word) to open up your output .txt file and edit your transcript for accuracy.\nHint: You can use Ctrl + A to highlight all of the text, and apply a preset format to make the document look even better\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"09da0f44a81145a185a93a82144b16bb","permalink":"/code/transcript-processing-template/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/code/transcript-processing-template/","section":"code","summary":"\u003c!DOCTYPE html\u003e\rTranscript Processing Template\rTranscript Processing Template\rLiz G\r9/16/2020\rInstalling R and R Studio\rTo use these functions, you need to install R. You may also want to install the free version of RStudio, as it it a little more user-friendly.","tags":null,"title":"","type":"code"},{"authors":null,"categories":null,"content":"I welcome collaboration, consultation requests, and any questions about my work.\neglenn2@uoregon.edu\nResearch Gate\nLinkedIn\nGoogle Scholar\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0d0a00090037bce0b3d820531d37b562","permalink":"/connect/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/connect/","section":"","summary":"I welcome collaboration, consultation requests, and any questions about my work.\neglenn2@uoregon.edu\nResearch Gate\nLinkedIn\nGoogle Scholar","tags":null,"title":"LET'S CONNECT","type":"page"}]